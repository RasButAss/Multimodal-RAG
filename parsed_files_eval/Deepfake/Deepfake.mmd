# Deepfake Generation and Detection: A Benchmark and Survey

Gan Pei\({}^{1}\)

Gan Pei and Jiangning Zhang contribute equally. \({}^{1}\) East China Normal University, Shanghai, China. \({}^{2}\) Youtu Lab, Tencent, Shanghai, China. \({}^{3}\) Nanjing University, Suzhou, China. \({}^{4}\) Shanghai Jiao Tong University, Shanghai, China. \({}^{5}\) Zhejiang University, Hangzhou, China. \({}^{6}\) The University of Sydney, Sydney, Australia.

Jiangning Zhang\({}^{2}\)

Gan Pei and Jiangning Zhang contribute equally. \({}^{1}\) East China Normal University, Shanghai, China. \({}^{2}\) Youtu Lab, Tencent, Shanghai, China. \({}^{3}\) Nanjing University, Suzhou, China. \({}^{4}\) Shanghai Jiao Tong University, Shanghai, China. \({}^{5}\) Zhejiang University, Hangzhou, China. \({}^{6}\) The University of Sydney, Sydney, Australia.

Menghan Hu\({}^{1}\)

Gan Pei and Jiangning Zhang contribute equally. \({}^{1}\) East China Normal University, Shanghai, China. \({}^{2}\) Youtu Lab, Tencent, Shanghai, China. \({}^{3}\) Nanjing University, Suzhou, China. \({}^{4}\) Shanghai Jiao Tong University, Shanghai, China. \({}^{5}\) Zhejiang University, Hangzhou, China. \({}^{6}\) The University of Sydney, Sydney, Australia.

Zhenyu Zhang\({}^{3}\)

Gan Pei and Jiangning Zhang contribute equally. \({}^{1}\) East China Normal University, Shanghai, China. \({}^{2}\) Youtu Lab, Tencent, Shanghai, China. \({}^{3}\) Nanjing University, Suzhou, China. \({}^{4}\) Shanghai Jiao Tong University, Shanghai, China. \({}^{5}\) Zhejiang University, Hangzhou, China. \({}^{6}\) The University of Sydney, Sydney, Australia.

Chengjie Wang\({}^{2}\)

Gan Pei and Jiangning Zhang contribute equally. \({}^{1}\) East China Normal University, Shanghai, China. \({}^{2}\) Youtu Lab, Tencent, Shanghai, China. \({}^{3}\) Nanjing University, Suzhou, China. \({}^{4}\) Shanghai Jiao Tong University, Shanghai, China. \({}^{5}\) Zhejiang University, Hangzhou, China. \({}^{6}\) The University of Sydney, Sydney, Australia.

Yunsheng Wu\({}^{2}\)

Gan Pei and Jiangning Zhang contribute equally. \({}^{1}\) East China Normal University, Shanghai, China. \({}^{2}\) Youtu Lab, Tencent, Shanghai, China. \({}^{3}\) Nanjing University, Suzhou, China. \({}^{4}\) Shanghai Jiao Tong University, Shanghai, China. \({}^{5}\) Zhejiang University, Hangzhou, China. \({}^{6}\) The University of Sydney, Sydney, Australia.

Guangtao Zhai\({}^{4}\)

Gan Pei and Jiangning Zhang contribute equally. \({}^{1}\) East China Normal University, Shanghai, China. \({}^{2}\) Youtu Lab, Tencent, Shanghai, China. \({}^{3}\) Nanjing University, Suzhou, China. \({}^{4}\) Shanghai Jiao Tong University, Shanghai, China. \({}^{5}\) Zhejiang University, Hangzhou, China. \({}^{6}\) The University of Sydney, Sydney, Australia.

Jian Yang\({}^{3}\)

Gan Pei and Jiangning Zhang contribute equally. \({}^{1}\) East China Normal University, Shanghai, China. \({}^{2}\) Youtu Lab, Tencent, Shanghai, China. \({}^{3}\) Nanjing University, Suzhou, China. \({}^{4}\) Shanghai Jiao Tong University, Shanghai, China. \({}^{5}\) Zhejiang University, Hangzhou, China. \({}^{6}\) The University of Sydney, Sydney, Australia.

Chunhua Shen\({}^{5}\)

Gan Pei and Jiangning Zhang contribute equally. \({}^{1}\) East China Normal University, Shanghai, China. \({}^{2}\) Youtu Lab, Tencent, Shanghai, China. \({}^{3}\) Nanjing University, Suzhou, China. \({}^{4}\) Shanghai Jiao Tong University, Shanghai, China. \({}^{5}\) Zhejiang University, Hangzhou, China. \({}^{6}\) The University of Sydney, Sydney, Australia.

Dacheng Tao\({}^{6}\)

Gan Pei and Jiangning Zhang contribute equally. \({}^{1}\) East China Normal University, Shanghai, China. \({}^{2}\) Youtu Lab, Tencent, Shanghai, China. \({}^{3}\) Nanjing University, Suzhou, China. \({}^{4}\) Shanghai Jiao Tong University, Shanghai, China. \({}^{5}\) Zhejiang University, Hangzhou, China. \({}^{6}\) The University of Sydney, Sydney, Australia.

Received: date / Accepted: date

###### Abstract

Deepfake is a technology dedicated to creating highly realistic facial images and videos under specific conditions, which has significant application potential in fields such as entertainment, movie production, and digital human creation, to name a few. With the advancements in deep learning, techniques primarily represented by Variational Autoencoders and Generative Adversarial Networks have achieved impressive generation results. More recently, the emergence of diffusion models with powerful generation capabilities has sparked a renewed wave of research. In addition to deepfake generation, corresponding detection technologies continuously evolve to regulate the potential misuse of deepfakes, such as privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing current state-of-the-arts in this rapidly evolving field. First, we unify task definitions, comprehensively introduce datasets and metrics, and discuss developing technologies. Then, we discuss the development of several related sub-fields and focus on researching four representative deepfake fields: face swapping, face reenactment, talking face generation, and facial attribute editing, as well as forgery detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and most influential published works. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in this project.

Keywords:Deepfake Generation Face Swapping Face Reenactment Talking Face Generation Facial Attribute Editing Forgery detection Survey ```

## 1 Introduction

Artificial Intelligence Generated Content (AIGC) garners considerable attention [227] in academia and industry. Deepfake generation, as one of the important technologies in the generative domain, gains significant attention due to its ability to create highly realistic facial media content. This technique transitions from traditional graphics-based methods to deep learning-based approaches. Early methods employ advanced Variational Autoencoder [136; 239; 255] (VAE) and Generative Adversarial Network (GAN) [125; 126] techniques, enabling seemingly realistic image generation, but their performance is still unsatisfactory, which limits practical applications. Recently, the diffusion structure [15; 77; 166] has greatly enhanced the generation capability of images and videos. Benefiting from this new wave of research, deepfake technology demonstrates potential value for practical applications and can generate content indistinguishable from real ones, which has further attracted attention and is widely applied in numerous fields [11], including entertainment, movie production, online live broadcasting, and privacy protection, _etc._

Deepfake generation can generally be divided into four mainstream research fields: 1) Face swapping [6; 236; 283] is dedicated to executing identity exchanges between two person images; 2) Face reenactment [17; 97; 140] emphasizes transferring source movements and poses; 3) Talking face generation [179; 189; 326] focuseson achieving natural matching of mouth movements to textual content in character generation, and 4) Facial attribute editing [105, 241, 210] aims to modify specific facial attributes of the target image. The development of related foundational technologies has gradually shifted from single forward GAN models [69, 125] to multi-step diffusion models [93, 223, 15] with higher quality generation capabilities, and the generated content has also gradually transitioned from single-frame images to temporal video modeling [77]. In addition, NeRF [66, 187] has been frequently incorporated into modeling to improve multi-view consistency capabilities [115, 330].

While enjoying the novelty and convenience of this technology, the unethical use of it raises concerns over the spread of privacy invasion, and dissemination of fake news, necessitating the development of effective forgery detection methods [249] in opposition, _a.k.a._, deepfake detection. From the earliest handcrafted feature-based methods [353, 90] to deep learning-based methods [304, 308], and the recent hybrid detection techniques [109], forgery detection has undergone substantial technological advancements along with the development of generative technologies. The data modality has also transitioned from the spatial and frequent domains [150, 219] to the more challenging temporal domain [84, 301]. Considering that current generative technologies have a higher level of interest, develop faster, and can generate indistinguishable content from reality [190], corresponding detection technologies need continuous evolution.

Overall, despite the significant progress made in both directions, they still exhibit challenging limitations in specific scenarios [257], mainly reflected in the visual perception authenticity and generative accuracy of models. This has attracted a large number of researchers to continue their efforts and has sparked thoughts on industrial applications. Existing survey works only focus on partial deepfake fields and lack discussions on new technologies [169, 190, 227], especially diffusion-based image/video generation methods, due to their disconnection from current technologies. The survey will comprehensively discuss these fields as well as related sub-fields, and cover tracking of the latest works.

\(\bullet\)**Contribution.** In this survey, we comprehensively explore the key technologies and latest advancements in Deepfakes generation and forgery detection. We first unify the task definitions (Sec. 2.1), provide a comprehensive comparison of datasets and metrics (Sec. 2.3), and discuss the development of related technologies. Specifically, we investigate four mainstream deepfake fields: face swapping (Sec. 3.1.1), face reenactment (Sec. 3.1.2), talking face generation (Sec. 3.1.3), and facial attribute editing (mainly on multiple editing) (Sec. 3.1.4), as well as forgery detection (Sec. 3.2). We also analyze the benchmarks and settings for each domain, thoroughly evaluating the latest and influential works published in top-tier conferences/journals (Sec. 4), especially recent diffusion-based approaches. Additionally, we discuss closely related fields, including head swapping, face super-resolution, face reconstruction, face inpainting, body animation, portrait style transfer, makeup transfer, and adversarial sample detection. Benefiting from the current popularity of AIGC, the research iteration cycle in the deepfake field has been significantly reduced, and we keep updating and discussing the in-submission works in the revised version.

\(\bullet\)**Scope.** This survey primarily focuses on mainstream face-related tasks, including face swapping, face reenactment, talking face generation, facial (multiple) attribute editing, and forgery detection. We also cover some related domain tasks in Sec. 2.4 and detail specific popular sub-tasks in Sec. 3.3. Considering the large number of articles (including published and preprints), we mainly include representative and attention-grabbing works. In addition, we compare this investigation with recent surveys. Sha \(et\)\(al.\)[227] only discuss character generation while we cover a more comprehensive range of tasks. Compared to works [169, 184, 190], our study encompasses a broader range of technical models, particularly the more powerful diffusion-based methods. Additionally,

Figure 1: Time diagram that reflects the survey pipeline. Zoom in for a better holistic perception of this work.

we thoroughly discuss the related sub-fields of deepfake generation and detection.

\(\bullet\)**Survey Pipeline.** Fig. 1 shows the pipeline of this survey. Sec. 2 provides the essential background knowledge that encompasses task-specific definitions, datasets, evaluation metrics, and exploration of related research areas. Sec. 3 presents technical discussions, examining the four most popular deepfake tasks from the perspective of technological categorization and evolution. In addition, we meticulously classify and discuss forgery detection techniques, emphasizing the technological routes involved. Subsequently, Sec. 4 organizes and evaluates performance from various approaches for a thorough and fair performance comparison. Sec. 5 focuses on a critical review of the challenges that persist in existing techniques, outlining feasible future development directions. Finally, we encapsulate the entirety of the paper through a comprehensive summary in Sec. 6.

## 2 Background

In this section, we first introduce the conceptual definitions of the discussed mainstream fields. Fig. 2 illustrates the intuitive objectives for each task and shows the distinctions among tasks in terms of manipulated facial components. Then, we review the developmental history of commonly used neural networks, highlighting several representative ones. Next, we summarize popular datasets, metrics, and loss functions. Finally, we comprehensively discuss several relevant domains.

### **Problem Definition**

\(\bullet\)**Unified Formulation of Studied Problems.** Fig. 2 intuitively displays the various deepfakes generation and detection tasks studied in this paper. For the former, different tasks can essentially be expressed as controlled content generation problems under specific conditions, such as images, audio, text, specific attributes, \(etc\).. Given the target image \(I_{t}\) to be manipulated and the condition information \(C=\{\texttt{Image},\texttt{Audio},\texttt{Text},\dots\}\), the content generation process can be represented as:

\[I_{o}=\mathbf{\phi_{G}}(I_{t},C), \tag{1}\]

where \(\phi_{G}\) abstracts the specific generation network and \(I_{o}=\{I_{t}^{0},I_{t}^{1},\dots,I_{t}^{N-1}\}\) represents generated contents. \(N\) is the total frame number for the generated video, which is set to 1 by default). The latter task can be viewed as an image-level or pixel-level classification problem as practical application needs, which can be represented as:

\[S_{o}=\mathbf{\phi_{D}}(I_{o}), \tag{2}\]

where \(\phi_{D}\) abstracts the specific detection network and \(S_{o}\) represents the fake score for the generated content \(I_{o}\).

\(\bullet\)**Face Swapping** involves replacing the identity information of the target face \(I_{t}\) by the source face \(C\)=\(I_{s}\), while preserving ID-irrelevant attributes of the target face, such as skin color and expressions.

\(\bullet\)**Face Reenactment** alters the facial movement of a target image \(I_{t}\) without changing the identity and other

Figure 2: Top: Illustration of different deepfake generation (Sec. 3.1) and detection tasks (Sec. 3.2) that are discussed in this survey. Bottom: Specific facial attribute modification of each task. Data from NVIDIA Keynote at COMPUTEX 2023 at 29:40.

facial attributes, under the conditions of driving image, video, head pose, _etc._. This technology often involves the support of facial motion capture technology, such as facial tracking or prediction based on deep learning models [346].

\(\bullet\)**Talking Face Generation** can be viewed as an extension in time, aiming at generating a talking video \(I_{o}\)=\(\{I_{o}^{i}\},i=0,1,\cdots,N-1\) with the character in the target image \(I_{t}\) engaging in a conversation based on an arbitrary driving source, such as text, audio, video, or a multi-modal composite source. The lip movements, facial poses, expressions, emotions, and spoken content information of the character in the generated video match the target information.

\(\bullet\)**Facial Attribute Editing** aims to modify the semantic information of the target face \(I_{t}\) (_e.g._, personality, age, expressions, skin color, _etc._) in a directed manner based on individual interest and preference. Existing methods include single and comprehensive attribute editing: the former focuses on training a model for only one attribute, while the latter integrates multiple attribute editing tasks simultaneously that is our primary focus in this survey.

\(\bullet\)**Forgery Detection** aims to detect and identify anomalies, tampering, or forgery areas in images or videos by the anomaly score \(S_{o}\), and it has great research and application value in information security and multimedia forensics.

### **Technological History and Roadmap**

\(\bullet\)**Generative Framework.** Variational Autoencoders (VAEs) [255, 239, 136], Generative Adversarial Networks (GANs) [125, 69, 126], and Diffusion [238, 93, 223] have played pivotal roles in the developmental history of generative models.

**1)** VAE [136] emerges in 2013, altering the relationship between latent features and linear mappings in autoencoder latent spaces. It introduces feature distributions like the Gaussian distribution and achieves the generation of new entities through interpolation. To enhance generation capabilities under specific conditions, CVAE [239] introduces conditional input. VQ-VAE [255] introduces the concept of vector quantization to improve the learning of latent representations. Subsequent models continually advance.

**2)** GANs [69] achieve high-quality generation through adversarial training with an extra discriminator. Subsequently, research on GANs experiences a surge, and currently, implementations based on GANs remain the mainstream approach for various deepfake tasks. CGAN [191] introduces conditional control variables to GANs. Pix2Pix [111] enhances GAN performance in specific image translation tasks. StyleGAN [125] introduces the concept of style transfer to GANs. StyleGAN2 [126] further improves the quality and controllability of generated images. Additionally, some composite research combines GANs with VAEs, e.g., CVAE-GAN [9].

**3)** Diffusion models [238] model the generation of data as a diffusion process. DDPM [93] gains widespread attention for its outstanding generative performance, especially when the model excels in handling large-scale, high-resolution images. LDM [223] is more flexible and powerful when it comes to modeling complex data distributions. In the field of video generation, diffusion models play a crucial role [164, 177, 81]. SVD [15] fine-tunes the base model using an image-to-video conversion task based on text-to-video models. AnimateDiff [77] attaches a newly initialized motion modeling module to a frozen text-to-image model and trains it on subsequent video clips to refine reasonable motion prior knowledge, thus achieving excellent generation results.

\(\bullet\)**Discriminative Neural Network.** Convolutional Neural Networks (CNNs) [144, 174, 329, 331, 89, 141] have played a pivotal role in the history of deep learning. LeNet [144], as the pioneer of CNNs, showcased the charm of machine learning. AlexNet [141] and ResNet [89] made deep learning feasible. Recently, ConvNeXt [174] has achieved excellent results surpassing those of Swin-Transformer [172]. The Transformer architecture initially proposed [256] in 2017. The core idea involves using self-attention mechanisms to capture dependencies between different positions in the input sequence, enabling global modeling of sequences. ViT [53]

Fig. 4: Works summarization on different directions per year. Data is obtained on 2024/05/13.

Fig. 3: Development timeline of three mainstream generative models, _i.e._, VAE, GAN, and Diffusion.

demonstrates that using Transformer in the field of computer vision can still achieve excellent performance and PVT [271] overcomes the challenges of adapting Transformer to various dense prediction tasks. In addition, Swin-Transformer [172] addresses the limitations of Transformer in handling high-resolution image processing tasks. Subsequently, Swin-Transformer V2 [170] further improves the model's efficiency and the resolution of manageable inputs.

\(\bullet\)**Neural Radiance Field** (NeRF) is first introduced in 2020 [187], with its core idea revolving around the use of volume rendering and implicit neural fields to represent and reconstruct both geometric and illumination information of 3D scenes [66]. Compared to traditional 3D methods, it exhibits higher visual quality and is currently widely applied in tasks such as 3D geometry enhancement [110, 228], segmentation [168] and 6D pose estimation [188]. In addition, Some notable works [108, 303] combining NeRF as a supplement to 3D information and generation models are particularly prominent at present.

\(\bullet\)**Work Summary.** The evolution of mainstream generative models is depicted chronologically in Fig. 3. This survey delves into four categories of generation tasks along with the forgery detection task, and the publication years distribution of the surveyed articles is shown in Fig. 4.

### **Datasets, Metrics, and Losses**

\(\bullet\)**Dataset.** Given the various datasets in surveyed fields, we use numerical labels to save post-textual space. **1)** Commonly used deepfake generation datasets include LFW [103], CelebA [173], CelebA-HQ [124], VGGFace [211], VGGFace2 [22], FFHQ [125], Multi-PIE [193], VoxCeleb1 [198], VoxCeleb2 [39], MEAD [263], MM CelebA-HQ [280], CelebAText-HQ [242], CelebV-HQ [359], TalkingHead-1KH [270], LRS2 [2], LRS3 [3], _etc._**2)** Commonly used forgery detection datasets include UADFV [156], Deepfake-TIMIT [138], FF++ [225], Deeperforensics-1.0 [117], DFDCP [51], DFDC [50], Celeb-DF [158], Celeb-DFv2 [158], FakeAVCeleb [130], DFD [48], WildDeepfake [363], KoDF [142], UADFV [302], Deephy [199], DF-Platter [200], _etc._ We summarize popular datasets in Table 1.

\(\bullet\)**Metric. 1)** For deepfake generation tasks, commonly used metrics include: Peak Signal-to-Noise Ratio (PSNR) [262], Structured Similarity (SSIM) [274], Learned Perceptual Image Patch Similarity (LPIPS) [336], Frechet Inception Distance (FID) [92], Kernel Inception Distance (KID) [12], Cosine Similarity (CSIM) [270], Identity Retrieval Rate (ID Ret) [258], Expression Error [28], Pose Error [226], Landmark Distance (LMD) around the mouths [32], Lip-sync Confidence (LSE-C) [217], Lip-sync Distance (LSE-D) [217], _etc._**2)** For forgery generation commonly uses: Area Under the ROC Curve (AUC) [143], Accuracy (ACC) [260], Equal Error Rate (EER) [102], Average Precision (AP) [249], F1-Score [35], _etc._ Detailed definitions are explained in Sec. 4.1.

\(\bullet\)**Loss Function.** VAE-based approaches generally employ reconstruction loss and KL divergence loss [136]. Commonly used reconstruction loss functions include Mean Squared Error, Cross-Entropy, LPIPS [336], and perceptual [122] losses. GAN-based methods further introduce adversarial loss [69] to increase image authenticity, while diffusion-based works introduce denoising loss function [93].

### **Related Research Domains**

\(\bullet\)**Head Swapping** replaces the entire head information rather than only the face of the target image [237], including facial contours and hairstyle, with that of the

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & Dataset & Type & Scale & Highlight \\ \hline \multirow{11}{*}{
\begin{tabular}{} \end{tabular} } & LVP [103] & Image & 10K & Facial images captured under various lighting conditions, poses, expressions, and occlusions at 250\(\times\)250 resolution. \\  & CelebA [172] & Image & 200K & The dataset includes over 200K facial images from the mean 10K individuals, each with 40 attribute labels. \\  & VGGFace [211] & Image & 2000K & A super-large-scale facial dataset involving a staggering 20K participants, encompassing a total of 2000K facial images. \\  & VoxCeleb1 [188] & Video & 100K videos & A large scale audio-visual dataset of human speech; the audio methods noisy background interference. \\  & CelebA-HQ [242] & Image & 300K & A high-resolution facial dataset consisting of 30K face images, each with a resolution of 102K1024 resolution. \\  & VGGFace2 [22] & Image & 3000K & A large scale facial dataset, expanded with greater diversity in terms of ethnicity and pose compared to VGGFace. \\  & VGGFace2 [239] & Video & 2K hours & A dataset that is five times larger in scale than VoxCeleb1 and improves facial diversity. \\  & FFHQ [125] & Image & 70K & The dataset with over 70K high-resolution (1024\(\times\)1024) facial images, advocating diverse ethnicity, age, and backgrounds. \\  & MRAD [283] & Video & 40 hours & An emotional adversarial dataset provides facial expression information during conversations with various emotional labels. \\  & CelebA-HQ [349] & Video & 30K videos & Videos close that a facial video clips have a resolution of more than 512\(\times\)512 resolution and are annotated with rich attribute labels. \\  & DesidFMT [114], Audio-Video & 640 videos & Video & The dataset is evenly divided into two versions: LQ (64\(\times\)64) and HQ (128\(\times\)128), with all videos using face swappinggery. \\  & EF+ [225] & Video & 640K videos & Combinging 1K original videos and manipulated videos generated using five different fingersy methods. \\  & DFDCP [51] & Audio-Video & 38K videos & The preliminary dataset for The Deepfake Detection Challenge includes two face-exwapping methods. \\  & DFDD [48] & Video & 34K videos & The dataset comprises 3K deformable videos generated using few larger methods. \\  & DFDCP [18] & Video & Video & Video feature feature with diverse skin tones, and rich environment diversity was considered during the filming process. \\  & DFDCP [51] & Audio-Video & 128K videos & The official dataset for The Deepfake Detection Challenge and contain a substantial amount of interference information. \\  & Celeb-DPF [184] & Video & 1K videos & Comprising 408 genuine videos from diverse age groups, chintage and gunders, along with 750 DePake videos. \\  & Celeb-DPF [125] & Video & 640K videos & An expanded version of Celeb-DFv1, this dataset not only increases in quantity but also the diversity. \\  & FakeAVCeleb [126] & Audio-Video & 200K videos & A novel audio-visual multimodal deepfake detection dataset, despfake videos generated using four forgery methods. \\ \hline \hline \end{tabular}
\end{table}
Table 1: Overview of commonly used datasets. Orange-marked ones are selected to evaluate different methods in Sec. 4.

source image. However, in terms of facial attributes, Head Swap only replaces identity attributes, while leaving other attribute information unchanged. Recently, some methods [87; 264] based on Diffusion have been proposed and have shown promising results in achieving good performance.

\(\bullet\)**Face Super-resolution** aims to enhance the resolution of low-resolution face images to generate high-resolution face images [196; 333]. This task is closely related to various deepfake sub-tasks. In the early stages of Face Swapping [201; 244] and Talking Face Generation [33; 58], there is consistently a low-resolution issue in the generated images and videos. This problem is addressed by incorporating face super-resolution methods into the models [189; 289] to improve output quality. In terms of technical approaches, FSR can be categorized into methods based on CNNs [30; 36; 104], GANs [137; 197], reinforcement learning [234], and ensemble learning [116].

\(\bullet\)**Face Reconstruction** refers to the process of recreating the three-dimensional appearance of an individual's face based on one or multiple 2D facial images [194; 229]. Facial reconstruction often plays an intermediate role in various deepfake sub-tasks. In Face Swapping and Face Reenactment tasks, facial parameters are reconstructed using 3DMM, and the model's parameters can be controlled directionally. Additionally, reconstructing a 3D facial model is one of the methods to address the issue of facial artifacts in videos generated under large facial poses. Common technical approaches for facial reconstruction include methods based on 3DMM [160; 180], epipolar geometry [61], one-shot learning [254; 281], shadow shape reconstruction [118; 128], and hybrid learning-based reconstruction [27; 54].

\(\bullet\)**Face Inpainting**, _a.k.a._ face completion, aims to reconstruct missing regions in face images caused by external factors such as occlusion and lighting while preserving facial texture information is crucial in this process [340]. This task is a crucial sub-task of image inpainting, and the current methods are mostly based on deep learning that can be roughly divided into two categories: GAN based [307; 352] and Diffusion based [286; 298].

\(\bullet\)**Body Animation** aims to alter the entire bodily pose while unchanging the overall body information [314]. The goal is to achieve a modification of the target image's entire body posture using an optional driving image or video, aligning the body of the target image with the information from the driving signal. The mainstream implementation path for body animation is based on GANs [343; 356], and Diffusion [177; 268; 273; 292].

\(\bullet\)**Portrait Style Transfer** aims to reconstruct the style of a target image to match that of a source image by learning the stylistic features of the source image [213; 299]. The goal is to preserve the content information of the target image while adapting its style to that of the source image [20]. Common applications include image cross-domain style transfer, such as transforming real face images into animated face styles [60; 216]. Methods based on GANs [1; 288; 120] and Diffusion [107; 163] have achieved high-quality performance in this task.

\(\bullet\)**Makeup Transfer** aims to achieve style transfer learning from a source image to a target image [119; 154]. Existing models have achieved initial success in applying and removing makeup on target images [167; 154; 26; 71], allowing for quantitative control over the intensity of makeup. However, they perform poorly in transferring extreme styles [295; 296; 349]. Existing mainstream methods are based on GANs [88; 119; 246].

\(\bullet\)**Adversarial Sample Detection** focuses on identifying whether the input data is an adversarial sample [86]. If recognized as such, the model can refuse to provide services for it, such as throwing an error or not producing an output [316]. Current deepfake detection models often rely on a single cue from the generation process as the basis for detection, making them vulnerable to specific adversarial samples. Furthermore, relatively little work has focused on adversarial sample testing in terms of model generalization capability and detection evaluation.

## 3 Deepfake Inspections: A Survey

This section systematically examines four generation tasks: Face Swapping (Sec. 3.1.1), Talking Face Generation (Sec. 3.1.3), Face Reenactment (Sec. 3.1.2), Facial Attribute Editing (Sec. 3.1.4). Additionally, we review techniques for forgery detection (commonly known as deepfake detection) in Sec. 3.2. We document and compare the most representative works in the main text. Furthermore, we discuss several related domains that garnered significant attention in Sec. 3.3.

### **Deepfake Generation**

#### 3.1.1 **Face Swapping**

In this section, we review face swapping methods from the perspective of basic architecture, which can be mainly divided into four categories and summarized in Table 2.

\(\bullet\)**Traditional Graphics.** As representative early implementations, traditional graphics methods in the 

[MISSING_PAGE_FAIL:7]

the need for similarities in pose and lighting between source and target images. They also face challenges like low image resolution, modification of target attributes, and poor performance in extreme lighting and occlusion scenarios.

\(\bullet\)**Generative Adversarial Network.** GAN-based methods aim to obtain realistic images generated through adversarial training between the generator and the discriminator, becoming the mainstream face swapping approach. According to different improvement objectives, methods can be classified into seven categories: **1)** Early GAN-based methods [9; 165; 192; 358] address issues related to the similarity of pose and lighting between source and target images. DepthNets [192] combines GANs with 3DMM to map the source face to any target geometry, not limited to the geometric shape of the target template. This allows it to be less affected by differences in pose between the source and target faces. However, they face challenges in generalizing the trained model to unknown faces.

**2)** Improved Generalizability. To improve the model's generalization, many efforts [203; 244; 34] are made to explore solutions. Combining GANs with VAEs, the model [10; 201] encodes and processes different facial regions separately. FSGAN [203] integrates face reenactment with face swap, designing a facial blending network to mix two faces seamlessly. SimSwap [34] introduces an identity injection module to avoid integrating identity information into the decoder. However, these methods suffer from low resolution and significant attribute loss and need help to handle facial occlusions effectively.

**3)** Resolution Upgrading. Some methods [114; 282; 361] provide solutions to enhance the resolution of generated images. MegaFS [361] introduces the first single-lens face swapping method at the million-pixel level. The face encoder no longer compresses facial information but represents it in layers, achieving more detailed preservation. StyleIPSB [114] constrains semantic attribute codes within the subspace of StyleGAN, thereby fixing certain semantic information during face swapping to preserve pore-level details.

**4)** Geometric Detail Preservation. To capture and reproduce more facial geometric details, some methods [220; 341; 320; 272] introduce 3DMM into GANs, enabling the incorporation of 3D priors. HifiFace [272] introduces a novel 3D shape-aware identity extractor, replacing traditional face recognition networks to generate identity vectors that include precise shape information. FlowFace [320] introduces a two-stage framework based on semantic guidance to achieve shape-aware face swapping. FlowFace++ [341] improves upon FlowFace by utilizing a pre-trained Mask Autoencoder to convert face images into a fine-grained representation space shared between the target and source faces. It further enhances feature fusion for both source and target by introducing a cross-attention fusion module. However, most of the aforementioned methods often struggle to effectively handle occlusion issues.

**5)** Facial Masking Artifacts. Some methods [152; 171; 203; 224] have partially alleviated the artifacts caused by facial occlusion. FSGAN [203] designs a restoration network to estimate missing pixels. E4S [171] redefines the face-swapping problem as a mask-exchanging problem for specific information. It utilizes a mask-guided injection module to perform face swapping in the latent space of StyleGAN. However, overall, the methods above have not thoroughly addressed the issue of artifacts in generated images under extreme occlusion conditions.

**6)** Trade-offs between Identity-replacement and Attribute-retention. In addition to the occlusion issues that need further handling, researchers [236; 65] discover that the balance between identity replacement and attribute preservation in generated images seems akin to a seesaw. Many methods [162; 293; 134] explore the equilibrium between identity replacement and attribute retention. InfoSwap [65] aims to decouple identity and attribute information in face swapping by leveraging the information bottleneck theory. It seeks controlled swapping of identity between source and target faces. StyleSwap [293] introduces a novel swapping guidance strategy, the ID reversal, to enhance the similarity of facial identity in the output. Shiohara _et al_. [236] propose BlendFace, using an identity encoder that extracts identity features from the source image and uses it as identity distance loss, guiding the generator to produce facial exchange results.

**7)** Model Light-weighting is also an important topic with profound implications for the widespread application of models. FastSwap [309] achieves this by innovating a decoder block called Triple Adaptive Normalization (TAN), effectively integrating identity information from the source image and pose information from the target image. XimSwap [6] modifies the design of convolutional blocks and the identity injection mechanism, successfully deploying on STM32H743.

\(\bullet\)**Diffusion-based.** The latest studies [345; 87; 135; 166] in this area produce promising generation results. DiffSwaps [345] redefines the face swapping problem as a conditional inpainting task. Liu _et al_. [166] introduce a multi-modal face generation framework and achieved this by introducing components such as balanced identity and expression encoders to the conditional diffusion model, striking a balance between identity replacement and attribute preservation during the generation process. As a novel facial generalist model, FaceX [87] can achieve various facial tasks, including face swapping and edit 

[MISSING_PAGE_FAIL:9]

mator addresses the regression of deformations related to 3D poses and expressions. The eye gaze estimator controls eye movement in videos, providing finer details. MetaPortrait [324] achieves accurate distortion field prediction through dense facial keypoint matching and accelerates model training based on meta-learning principles, delivering excellent results on limited datasets.

\(\bullet\)**Feature Decoupling.** The latent feature decoupling and driving methods [16; 17; 18; 155; 300] aims to disentangle facial features in the latent space of the driving video, replacing or mapping the corresponding latent information to achieve high-fidelity facial reproduction under specific conditions. HyperReenact [17] uses attribute decoupling, employing a hyper-network to refine source identity features and modify facial poses. StyleMask [18] separates facial pose and expression from the identity information of the source image by learning masks and blending corresponding channels in the pre-trained style space S of StyleGAN2. HiDe-NeRF [155] employs a deformable neural radiance field to represent a 3D scene, with a lightweight deformation module explicitly decoupling facial pose and expression attributes.

\(\bullet\)**Self-supervised Learning.** Self-supervised learning employs supervisory signals inferred from the intrinsic structure of the data, reducing the reliance on external data labels [207; 253; 321; 328]. Oorloff _et al_. [207] employs self-supervised methods to train an encoder, disentangling identity and facial attribute information of portrait images within the pre-defined latent space itself of a pre-trained StyleGAN2. Zhang _et al_. [328] utilizes 3DMM to provide geometric guidance, employs pre-computed optical flow to guide motion field estimation, and relies on pre-computed occlusion maps to guide the perception and repair of occluded areas.

#### 3.1.3 **Talking Face Generation**

In this section, we review current methods from three perspectives: audio/text driven, multimodal conditioned, diffusion-based, and 3D-model Technologies. We also summarize them in Table 4.

\(\bullet\)**Audio/Text Driven.** Methods aim to map and guide lip and facial movements in generated videos by understanding the semantic information from the driving source [259; 332; 230]. Early methods [32; 58] perform poorly in terms of generalization and training complexity. After training, the models struggled to generalize to new individuals, requiring extensive conversational data for training new characters. Researchers [33; 217] propose their solutions from various perspectives. However, Most of these methods prioritize generating lip movements aligned with semantic information, overlooking essential aspects like identity and style, such as head pose changes and movement control, which are crucial in natural videos. To address this, MakeItTalk [357] decouples input audio information by predicting facial landmarks based on audio and obtaining semantic details on facial expressions and poses from audio signals. SadTalker [337] extracts 3D motion coefficients for constructing a 3DMM from audio and uses this to modulate a new 3D perceptual facial rendering for generating head poses in talking videos. Additionally, some methods [262; 145; 63; 279] propose their improvement methods, and these will not be detailed one by one. In addition, the emotional expression varies for different texts during a conversation, and vivid emotions are an essential part of real talking face videos [64; 233]. Recently, some methods [322; 250; 322] extend their previous approaches by incorporating matching between the driving information and corresponding emotions. EMMN [250] establishes an organic relationship between emotions and lip movements by extracting emotion embeddings from the audio signal, synthesizing overall facial expressions in talking faces rather than focusing solely on audio for facial expression synthesis. AMIGO [322] employs a sequence-to-sequence cross-modal emotion landmark generation network to generate vivid landmarks aided by audio information, ensuring that lips and emotions in the output image sequence are synchronized with the input audio. However, existing methods still lack effective control over the intensity of emotions. In addition, TalkCLIP [178] introduces style parameters, expanding the style categories for text-guided talking video generation. Zhong _et al_. [348] propose a two-stage framework, incorporating appearance priors during the generation process to enhance the model's ability to preserve attributes of the target face. DR2 [326] explores practical strategies for reducing the training workload.

\(\bullet\)**Multimodal Conditioned.** To generate more realistic talking videos, some methods [159; 261; 284; 351] introduce additional modal information on top of audio-driven methods to guide facial pose and expression. GC-AVT [159] generates realistic talking videos by independently controlling head pose, audio information, and facial expressions. This approach introduces an expression source video, providing emotional information during the speech and the pose source video. However, the video quality falls below expectations, and it struggles to handle complex background changes. Xu _et al_. [284] integrate text, image, and audio-emotional modalities into a unified space to complement emotional content in textual information. Multimodal approaches have significantly enhanced the vividness of generated videos, but there is still room for exploration of organically combining information driven by different sources and modalities.

\(\bullet\)**Diffusion-based.** Recently, some methods [285, 179, 189, 285, 325] apply the Diffusion model to the task of talking face generation. For fine-grained talking video generation, DAE-Talker [57] replaces manually crafted intermediate representations, such as facial landmarks and 3DMM coefficients, with data-driven latent representations obtained from a Diffusion Autoencoder (DAE). The image decoder generates video frames based on predicted latent variables. EmoTalker [325] utilizes a conditional diffusion model for emotion-editable talking face generation. It introduces emotion intensity blocks and the FED dataset to enhance the model's understanding of complex emotions. Very recently, diffusion models are gaining prominence in talking face generation tasks [240, 275, 252] and video generation tasks [123, 273]. Emo [252] directly predicts video from audio without the need for intermediate 3D components, achieving excellent results. However, the lack of explicit control signals may easily lead to unnecessary artifacts. Based on the Diffusion Transformer architecture, VASA-1 [287] finely encodes and reconstructs facial details, constructing an expressive and well-decoupled facial latent space. This results in highly controllable and high-resolution video generation.

\(\bullet\)**3D-model Technologies.** 3D models, exemplified by NeRF, are gaining traction in talking face generation [76, 148, 230]. AD-NeRF [76] directly feeds features from the input audio signal into a conditional implicit function to generate a dynamic NeRF. AE-NeRF [148] employs a dual NeRF framework to separately model audio-related regions and audio-independent regions. Furthermore, some methods [214, 306] adopt Tri-Plane [25] to represent facial and audio attributes. Synctalk [214] models and renders head motion using a tri-plane hash representation, and then further enhances the output quality using a portrait synchronization generator. Very recently, 3D Gaussian Splatting [129] also been widely applied to this task. Some method [37, 311, 151, 29] introduce 3DGS to achieve more refined facial reconstruction and motion details, aiming to address the issue of insufficient pose and expression control caused by NeRF's implicit representation.

#### Facial Attribute Editing

In this section, we review current methods chronologically, following the progression in overcoming technical challenges, primarily focusing on multiple attribute edit

[MISSING_PAGE_FAIL:12]

generation and editing. DiffusionRig [49] conditions the initial 3D face model, which helps preserve facial identity information during personalized editing of facial appearance based on the general facial details before the dataset.

### **Forgery Detection**

In this section, we review current forgery detection techniques based on the type of detection cues, categorizing them into three: Space Domain (Sec. 3.2.1), Time Domain (Sec. 3.2.2), Frequency Domain (Sec. 3.2.3), and Data-Driven (Sec. 3.2.4). We also summarize the detailed information about popular methods in Table 6.

#### 3.2.1 **Space Domain**

\(\bullet\)**Image-level Inconsistency.** The generation process of forged images often involves partial alterations rather than global generation, leading to common local differences in non-globally generated forgery methods. Therefore, some methods focus on differences in image spatial details as criteria for determining whether an image is forged, such as color [90], saturation [183], artifacts [342; 21; 32], gradient variations [249], _etc._ Specifically, RECCE [21] considers shadow generation from a training perspective, utilizing the learned representations on actual samples to identify image reconstruction differences. LGrad [249] utilizes a pre-trained transformation model, converting images to gradients to visualize general artifacts and subsequently classifying based on these representations. In addition, some works focus on detection based on differences in facial and non-facial regions [206], as well as the fine-grained details of image textures [175; 24]. Recently, Ba _et al_. [8] focus not only on the discordance in a single image region but also on the detection of fused local representation information from multiple non-overlapping areas.

\(\bullet\)**Local Noise Inconsistency.** Image forgery may involve adding, modifying, or removing content in the image, potentially altering the noise distribution in the image. Detection methods based on noise aim to identify such local or even global differences in the image. Zhou _et al_. [353] propose a dual-stream structure, combining GoogleNet with a triplet network to focus on tampering artifacts and local noise in images. Nguyen _et al_. [202] utilize capsule networks to detect forged pictures and videos in various forgery scenarios. NoiseDF [267] specializes in identifying underlying noise traces left behind in Deepfake videos, introducing an efficient and novel Multi-Head Relative Interaction with depth-wise separable convolutions to enhance detection performance.

#### 3.2.2 **Time Domain**

\(\bullet\)**Abnormal Physiological Information.** Forgery videos often overlook the authentic physiological features of humans, failing to achieve overall consistency with authentic individuals. Therefore, some methods focus on assessing the plausibility of the physiological features of the generated faces in videos. Li _et al_. [156] detect blinking and blink frequency in videos as criteria for determining the video's authenticity. Yang _et al_. [302] focuses on the inconsistency of head poses in videos, comparing the differences between head poses estimated using all facial landmarks and those estimated using only the landmarks in the central region. Peng _et al_. [212] focuse on inter-frame gaze angles, obtaining gaze characteristics of each video frame and using a spatio-temporal feature aggregator to combine temporal gaze features, spatial attribute features, and spatial texture features as the basis for detection and classification.

\(\bullet\)**Inter-Frame Inconsistency.** Methods [38; 347; 308; 304; 308] based on inter-frame inconsistency for forgery detection aim to uncover differences in images between adjacent frames or frames with specific temporal spans. Gu _et al_. [72] focuse on inter-frame image inconsistency by densely sampling adjacent frames, while Yin _et al_. [308] design a Dynamic Fine-grained Difference Capturing module and a Multi-Scale Spatio-Temporal Aggregation module to cooperatively model spatio-temporal inconsistencies. Yang _et al_. [304] approach forgery detection as a graph classification problem, emphasizing the relationship information between facial regions to capture the relationships among local features across different frames. Choi _et al_. [38] discover that the style variables in each frame of Deepfake work change. Based on this, they developed a style attention module to focus on the inconsistency of the style latent variables between frames.

\(\bullet\)**Multimodal Inconsistency.** The core idea behind multimodal detection algorithms is to make judgments based on the flow of prior information from multiple attributes rather than solely considering the image or audio differences of individual characteristics in each frame. The consideration of audio-visual modal inconsistency has received extensive research in various methods [301; 41; 59; 84]. POI-Forensics [41] proposes a deep forgery detection method based on audio-visual authentication, utilizing contrastive learning to learn the most distinctive embeddings for each identity in moving facial and audio segments. AVoiD-DF [301] embeds spatiotemporal information in a spatiotemporal encoder and employs a multimodal joint decoder to fuse multi-modal features and learn their inherent relationships. Subsequently, a cross-modal classifier is applied to detect disharmonious operations within and between modalities. Agarwal _et al_. [5] describe a forensic technique for detecting fake faces using static and dynamic auditory ear characteristics. Indeed, multimodal detection methods are currently a hotspot in forgery detection research.

#### 3.2.3 **Frequency Domain**

Frequency domain-based forgery detection methods transform image time-domain information into the frequency domain. Works [150; 248; 185; 62; 185] utilize statistical measures of periodic features, frequency components, and frequency characteristic distributions, either globally or in local regions, as evaluation metrics for forgery detection. Specifically, F\({}^{3}\)-Net [219] proposes a dual-branch framework. One frequency-aware branch utilizes Frequency-aware Image Decomposition (FAD) to learn subtle forgery patterns in suspicious images. In contrast, the other branch aims to extract high-level semantics from Local Frequency Statistics (LFS) to describe the frequency-aware statistical differences between real and forged faces. HFI-Net [185] consists of a dual-branch network and four Global-Local Interaction (GLI) modules. It effectively explores multi-level frequency artifacts, obtaining frequency-related forgery clues for face detection. Tan _et al_. [248] introduce a novel frequency-aware approach called FreqNet, which focuses on the high-frequency information of images and combines it with a frequency-domain learning module to learn source-independent features. Furthermore, some approaches combine spatial, temporal, and frequency domains for joint consideration [80; 181], and Guo _et al_. [80] design a spatial-frequency interaction convolution to construct a novel backbone network for Deepfake detection.

#### 3.2.4 **Data Driven**

Data-driven forgery detection focuses on learning specific patterns and features from extensive image or video

\begin{table}
\begin{tabular}{c c c c c c} \hline \multicolumn{2}{c}{Method} & Venue & Train & \multicolumn{2}{c}{Test} & \multicolumn{1}{c}{Highlight} \\ \hline Gram-Net [175] & CVPR-20 & \(\mathcal{D}\) & \(\mathcal{D}\) & The method posits that genuine faces and fake faces exhibit inconsistencies in texture details. & \\ Face X-ray [185] & CVPR-20 & \(\mathcal{D}\) & \(\mathcal{D}\) & Focusing on boundary artifacts of face fusion forgery detection. & \\ Zhao _et al_. [94] & CVPR-21 & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{A}\) texture enhancement module, an attention generation module, and a bilinear attention pooling mod & -ule are proposed to focus on texture details. & \\ Nvidiu _et al_. [206] & TPAMI-21 & \(\mathcal{D}\) & \(\mathcal{D}\) & Detecting swapped faces by comparing the facial region with its context (non-facial area). & \\ SIB [235] & CVPR-22 & \(\mathcal{D}\) & \(\mathcal{D}\) & The belief that the more difficult to detect forged faces typically contain more generalized traces of forgery & -ery can encourage the model to learn a feature representation with greater generalization ability. & \\  & RECCE [21] & CVPR-22 & \(\mathcal{D}\) & \(\mathcal{D}\) & Reconstruction learning on real samples to learn common compressed representations of real images. & \\  & LGGrad [249] & CVPR-23 & \(\mathcal{D}\) & \(\mathcal{D}\) & The gradient is utilized to present generalized artifacts that are fed into the classifier to determine the truth of the image. & \\  & NoiseDF [267] & AAAT23 & \(\mathcal{D}\) & \(\mathcal{D}\) & Extracting noise traces and features from cropped faces and background squares in video frames. & \\  & Ba _et al_. [8] & AAAT24 & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) & Multiple non-overlapping local representations are extracted from the image for forgery detection. A \\  & & & & local information loss function, based on information bottleneck theory, is proposed for constraint. & \\ \hline \multirow{7}{*}{} & Yang _et al_. [302] & ICASSP’19 & \(\mathcal{D}\) & \(\mathcal{D}\) & Focusing on the inconsistency in the head pose in videos by comparing the estimated head pose using all facial landmarks with the one estimated using only the landmarks in the central region. & \\  & FTCN [347] & ICCV’21 & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) & 152 \\  & LipForensics [85] & CVPR-21 & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) & Concern about temporal inconsistency of mouth movements in videos. & \\  & M2TR [260] & ICMR’22 & \(\mathcal{D}\) & \(\mathcal{D}\) & Capturing local inconsistencies at different scales forgery detection using a multiscale transformer. & \\  & Gu _et al_. [72] & AAAT22 & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) & By densely sampling adjacent frames to pay attention to the inter-frame image inconsistency. & \\  & Yang _et al_. [304] & TIFS’23 & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) & Treating detection as a graph classification problem and focusing on the relationship between the local image features across different frames. & \\  & AVoD-DF [301] & TIFS’23 & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) & Multimodal forgery detection using audiovisual inconsistency. & \\  & Choi _et al_. [88] & CVPR’24 & \(\mathcal{D}\) & \(\mathcal{D}\) & Focus on the inconsistency of the style latent vectors between frames. & \\  & Xu _et al_. [296] & IJCV’24 & \(\mathcal{D}\) & \(\mathcal{D}\) & Focus on inter-frame gaze angles, extracting gaze informations and employing spatio-temporal feature aggregation to combine temporal, spatial, and texture features for detection and classification. & \\  & Peng _et al_. [212] & TIFS’24 & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) & A two-branch frequency perception framework with a cross-attention module is proposed. & \\ \cline{2-6}  & F\({}^{3}\)-Net [219] & ECCV’20 & \(\mathcal{D}\) & \(\mathcal{D}\) & Propose an adaptive frequency feature generation module to extract different features from different frequency bands in a learnable manner. & \\  & FDFL [150] & CVPR’21 & \(\mathcal{D}\) & \(\mathcal{D}\) & Notice that the forgery flaws used to distinguish between real and fake faces are concentrated in the mid- and high-frequency spectrum. & \\  & HFI-Net [185] & TIFS’23 & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) & Using a backbone network for Deepfake detection with space-frequency interaction convolution. & \\  & Guo _et al_. [80] & TIFS’23 & \(\mathcal{D}\) & \(\mathcal{D}\) & A lightweight frequency-domain learning network is proposed to constrain classifier operation within the frequency domain. & \\  & Tan _et al_. [248] & AAAT24 & \(\mathcal{D}\) & \(\mathcal{D}\) & Utilizing attention mechanisms to handle the feature maps of the detection model. & \\ \hline \multirow{7}{*}{} & Dang _et al_. [45] & CVPR’20 & \(\mathcal{D}\) & \(\mathcal{D}\) & Propose pairwise self-consistent learning for training CNN to extract these source features and detect deep vacation images. & \\  & Zhao _et al_. [944] & ICCV’21 & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) \\ \cline{1-1}  & Finfer [90] & AAAT22 & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) & [66]. \\ \cline{1-1}  & Huang _et al_. [102] & CVPR’23 & \(\mathcal{D}\) & \(\mathcal{D}\) & \(\mathcal{D}\) & [152]. A new implicit identity-driven face exchange detection framework is proposed. & \\ \cline{1-1}  & Hiri-Net [75] & CVPR’23 & \(\mathcal{D}\) & \(\mathcal{D}\) & Converting forgery detection and localization into a hierarchical fine-grained classification problem. & \\ \cline{1-1}  & Zhai _et al_. [232] & ICCV’23 & \(\mathcal{D}\) & \(\mathcal{D}\) & [270] & Weakly supervised image processing detection is proposed such that only binary image level labels (real or tampered) are required for training. & \\ \hline \end{tabular}
\end{table}
Table 6: Overview of representative forgery detection methods. Notations: \(\mathcal{A}\) FF++, \(\mathcal{D}\) DFDC, \(\mathcal{C}\) Celeb-DF, \(\mathcal{A}\) Celeb-DF, \(\mathcal{A}\) Self-build, \(\mathcal{B}\) UADFV, \(\mathcal{C}\) Celeb-HQ, \(\mathcal{D}\) DFDCp, \(\mathcal{A}\) FFHQ, \(\mathcal{D}\) DFD.

datasets to distinguish between genuine and potentially manipulated images. Some methods [96; 313] believe that images generated by specific models possess unique model fingerprints. Based on this belief, forgery detection can be achieved by focusing on the model's training. In addition, FakeSpotter [265] introduces the Neuron Coverage Criterion to capture layer-wise neuron activation behavior. It monitors the neural behavior of a deep face recognition system through a binary classifier to detect fake faces. There are also methods [102; 344] that attempt to classify the sources of different components in an image. For instance, Huang _et al_. [102] think that the difference between explicit and implicit identity helps detect face swapping. There are numerous data-driven methods [45; 79; 99], and it is not feasible to discuss each one in detail here.

### Specific Related Domains

In this section, we briefly review related popular tasks beyond the deepfake generation, such as Face Super-resolution (Sec. 3.3.1), Portrait Style Transfer (Sec. 3.3.2), Body Animation (Sec. 3.3.3), and Makeup Transfer (Sec. 3.3.4).

#### 3.3.1 Face Super-resolution

\(\bullet\)**Convolutional Neural Networks.** Early works [101; 36; 176] on facial super-resolution based on CNNs aims to leverage the powerful representational capabilities of CNNs to learn the mapping relationship between low-resolution and high-resolution images from training samples. Depending on whether they focus on local details of the image, they can be divided into global methods [104; 36], local methods [101], and mixed methods [176].

\(\bullet\)**Generative Adversarial Network.** GAN aims to achieve the optimal output result through an adversarial process between the generator and the discriminator. This type of method [209; 338] currently dominates the field for flexible and efficient architecture.

#### 3.3.2 Portrait Style Transfer

\(\bullet\)**Generative Adversarial Network.** The most mature style transfer algorithm is the GAN-based approach [1; 288; 120]. However, due to the relatively poor stability of GANs, it is common for the generated images to contain artifacts and unreasonable components. 3DAvatarGAN [1] bridges the pre-trained 3D-GAN in the source domain with the 2D-GAN trained on an artistic dataset to achieve cross-domain generation. Scemirefy [120] utilizes semantic constraints provided by text models like CLIP to guide StyleGAN generation and applies patch-based contrastive style loss to enhance stylization and fine details further.

\(\bullet\)**Diffusion-based** methods [107; 146; 163] represent the generative process of cross-domain image transfer using diffusion processes. DiffusionGAN3D [146] combines 3DGAN [25] with a diffusion model from text to graphics, introducing relative distance loss and learnable tri-planes for specific scenarios to further enhance cross-domain transformation accuracy.

#### 3.3.3 Body Animation

\(\bullet\)**Generative Adversarial Network.** GAN-based approaches [356; 343; 112] aim to train a model to generate images whose conditional distribution resembles the target domain, thus transferring information from reference images to target images. CASD [356] is based on a style distribution module using a cross-attention mechanism, facilitating pose transfer between source semantic styles and target poses. VGFlow [112] introduces a visibility-guided flow module to preserve texture and perform style manipulation concurrently. However, existing methods still rely considerably on training samples, and exhibit decreased performance when dealing with actions in rare poses.

\(\bullet\)**Diffusion-based.** The task of body animation using diffusion models aims to utilize diffusion processes to generate the propagation and interaction of movements between body parts based on a reference source. This approach [100; 177; 273] represents a current hot topic in research and implementation. LEO [273] focuses on the spatiotemporal continuity between generated actions, employing the Latent Motion Diffusion Model to represent motion as a series of flow graphs during the generation process. Animate Anyone [100] harnesses the powerful generation capabilities of stable diffusion models combined with attention mechanisms to generate high-quality character animation video.

#### 3.3.4 Makeup Transfer

\(\bullet\)**Graphics-based Approaches.** Before the introduction of neural networks, traditional computer graphics methods [147; 73] use image gradient editing and physics-based operations to understand the semantics of makeup. By decomposing the input image into multiple layers, each representing different facial information, traditional methods would distort the reference facial image onto the non-makeup target using facial landmarks for each layer. However, due to the limitations of manually designed operators, the output images from traditional methods often appear unnatural, with noticeable artifacts. Additionally, there is a tendency for background information to be modified to some extent.

\(\bullet\)**Generative Adversarial Network.** Early deep learning-based methods [71] aim at fully automatic makeup transfer. However, these methods [26; 31] exhibit poor performance when faced with significant differences in pose and expression between the source and target faces and are unable to handle extreme makeup scenarios well. Some methods [119; 167; 349] proposes their solutions, PSGAN++ [167] comprises the Makeup Distillation Network, Attentive Makeup Morphing module, Style Transfer Network, and Identity Extraction Network, further enhancing the ability of PSGAN [119] to perform targeted makeup transfer with detail preservation. ELeGANt [296], CUMTGAN [88], and HT-ASE [154] explore the preservation of detailed information. ELeGANt [296] encodes facial attributes into pyramid feature maps to retain high-frequency information. Matching facial semantic information, which involves rendering makeup styles onto semantically corresponding positions of a target image, is often overlooked. SSAT [245] introduces the SSCFT module and weakly supervised semantic loss for accurate semantic correspondence. SSAT++ [246] further improves color fidelity matching, but both models are complex with high training complexity. In addition, BeautyREC [295], based on the Transformer with long-range visual dependencies, achieves efficient global makeup transfer with significantly reduced overall model parameters compared to previous works.

## 4 Benchmark Results

We firstly introduce the evaluation metrics commonly used for each deepfake tasks, and then we evaluate the performance of representative methods for each reviewed field on the most widely used datasets with the data sourced from the original respective papers. Considering the differences in training datasets, testing datasets, and metrics used by different approaches, we strive to compare them fairly in each table to the greatest extent.

### **Metrics**

\(\bullet\)**Face Swapping.** The most commonly used objective evaluation metrics for face swapping include ID Ret, Expression Error, Pose Error, and FID. ID Ret is calculated by a pre-trained face recognition model [258], measuring the Euclidean distance between the generated face and the source face. A higher ID Ret indicates better preservation of identity information. Expression and pose errors quantify the differences in expression and pose between the generated face and the source face. These metrics are evaluated using a pose estimator [226] and a 3D facial model [47], extracting expression and pose vectors for the generated and source faces. Lower values for expression error and pose error indicate higher facial expression and pose similarity between the swapped face and the source face. FID [92] is used to assess image quality, with lower FID values indicating that the generated images closely resemble authentic facial images in appearance.

\(\bullet\)**Face Reenactment.** Face reenactment commonly uses consistent evaluation metrics, including CSIM, SSIM [274], PSNR, LPIPS [336], LMD [32], and FID. CSIM describes the cosine similarity between the generated and source faces, calculated by ArcFace [46], with higher values indicating better performance. SSIM, PSNR, LPIPS, and FID are used to measure the quality of synthesized images. SSIM measures the structural similarity between two images, with higher values indicating a closer resemblance to natural images. PSNR quantifies the ratio between a signal's maximum possible power and noise's power, indicating higher quality for higher values. LPIPS assesses reconstruction fidelity using a pre-trained AlexNet [141] to extract feature maps for similarity score computation. As mentioned earlier, FID is used to evaluate image quality. LMD assesses the accuracy of lip shape in generated images or videos, with lower values indicating better model performance.

\(\bullet\)**Talking Face Generation.** Expanding upon face reenactment metrics, talking face generation incorporates additional metrics, including M/F-LMD, Sync, LSE-C, and LSE-D. The LSE-C and LSE-D are usually used to measure lip synchronization effectiveness [217]. The landmark distances on the mouth (M-LMD) [33] and the confidence score of SyncNet (Sync) measure synchronization between the generated lip motion and the input audio. F-LMD computes the difference in the average distance of all landmarks between predictions and ground truth (GT) as a measure to assess the generated expression. In addition, there are some meaningful metrics, such as LSE-C and LSE-D for measuring lip synchronization effectiveness [217], AVD [155] for evaluating identity preservation performance, AUCON [83] for assessing facial pose and expression jointly, and AGD [55] for evaluating eye gaze changes. These newly proposed evaluation metrics enrich the performance assessment system by targeting various aspects of the model's performance.

\(\bullet\)**Facial Attribute Editing.** The standard evaluation metrics used in face attribute manipulation are FID, LPIPS [336], KID [12], PSNR and SSIM. KID is one of the image quality assessment metrics commonly used in face editing work and other generative modeling tasks to quantify the difference in distribution between the generated image and the actual image, with lower KID values indicating better model performance. Some text-guide work will also use the CLIP Score to measure the consistency between the output image and the text, calculated as the cosine similarity between the normalized image and the text embedding. Higher values of CLIP Score indicate better consistency of the generated image with the corresponding text sentence.

### Main Results on Deepfake Generation

We select widely used metrics for each task and tabulate the performance for representative methods.

\(\bullet\)**Results on Face Swapping.** Table 7 displays the performance evaluation results of some representative models on the Face Swapping task using the FF++ [225] dataset. WSC-Swap [220] captures facial region external attribute information and internal identity information through two independent encoders, which achieves good identity preservation and facial pose retention. However, it exhibits sub-optimal performance in facial expression error metrics.

\(\bullet\)**Results on Face Reenactment.** Table 8 and Table 9 show the performance evaluation results on the VoxCeleb [198] dataset for self-reenactment and cross-subject reenactment, respectively. AVFR-GAN [4] achieves better performance by using the multimodal modeling. 10 presents quality assessment results on the VoxCeleb2 dataset. HiDe-NeRF [155] represents 3D scenes using canonical appearance fields and implicit deformation fields. It achieves accurate facial attribute modeling by explicitly decoupling facial pose and expression attributes using the deformation module.

\(\bullet\)**Results on Talking Face Generation.** Table 11 displays the performance results of various talking face generation approaches on the MEAD [263] dataset since 2023. AMIGO [322] achieves promising results among all methods, which utilizes seq2seq to generate facial landmarks for emotion tagging, matching mouth movements with emotional features. Additionally, it employs a landmark-to-image translation network to create facial images with fine textures.

\(\bullet\)**Results on Facial Attribute Editing.** Table 12 evaluates the quality level of generated images using FID, and Table 13 assesses facial reconstruction capabilities using PSNR, LPIPS, and SSIM. Due to different training and testing datasets, quantitative fair comparisons are not possible that just serve as performance demonstrations.

\begin{table}
\begin{tabular}{c|c|c c c c} \hline \hline  & & \multicolumn{4}{c}{Test: VoxCeleb2} \\ \cline{3-6} Methods & Train & CSIM\(\uparrow\) & PSNR\(\uparrow\) & LMD\(\downarrow\) & FID\(\downarrow\) & SSIM\(\uparrow\) \\ \hline PC-AVS [51] & \(\mathbf{\texttt{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0} \pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\mathbf{\color[rgb]{0,0,0} \definecolor[named]{

[MISSING_PAGE_FAIL:18]

authenticity, and security. Many approaches struggle to balance lightweight deployment and generating high-quality reenactment effects, hindering the widespread adoption of facial reenactment technology in industries. Moreover, several methods claim to achieve high-quality facial reenactment, but they exhibit visible degradation in output during rapid pose changes or extreme lighting conditions in driving videos. Additionally, the computational complexity, consuming significant time and system resources, poses substantial challenges for practical applications.

\(\bullet\)**Talking Face Generation.** Current methods strive to enhance the realism of generated conversational videos. However, they lack fine-grained control over the emotional nuances of the conversation, where the matching of emotional intonation to audio and semantic content is not precise enough, and the control over emotional intensity is too coarse. In addition, the realistic correlation between head pose and facial expression movement seems insufficiently considered. Lastly, for text or audio with intense emotions, noticeable artifacts in head movement still occur to a significant extent.

\(\bullet\)**Facial Attribute Editing.** Currently, mainstream facial attribute editing employs the decoupling concept based on GANs and diffusion models are gradually being introduced into this field. The primary challenge is effectively separating the facial attributes to prevent unintended processing of other facial features during attribute editing. Additionally, there needs to be a universally accepted benchmark dataset and evaluation framework for fair assessments of facial editing.

\(\bullet\)**Forgery Detection.** With the rapid development of facial forgery techniques, the central challenge in face forgery detection technology is accurately identifying various forgery methods using a single detection model. Simultaneously, ensuring that the model exhibits robustness when detecting forgeries in the presence of disturbances such as compression is crucial. Most detection models follow a generic approach targeting common operational steps of a specific forgery method, such as the integration phase in face swapping or assessing temporal inconsistencies, but this manner limits the model's generalization capabilities. Moreover, as forgery techniques evolve, forged videos may evade detection by introducing interference during the detection process.

## 6 Conclusion

This survey comprehensively reviews the latest developments in the field of deepfake generation and detection, which is the first to cover a variety of related fields thoroughly and discusses the latest technologies such as diffusion. Specifically, this paper covers an overview of basic background knowledge, including concepts of research tasks, the development of generative models and neural networks, and other information from closely related fields. Subsequently, we summarize the technical approaches adopted by different methods in the mainstream four generation and one detection fields, and classify and discuss the methods from a technical perspective. In addition, we strive to fairly organize and benchmark the representative methods in each field. Finally, we summarize the current challenges and future research directions for each field.

**Acknowledgement.** This work is supported by the National Natural Science Foundation of China (No. 62371189).

## References

* (1) Abdal, R., Lee, H.Y., Zhu, P., Chai, M., Siarohin, A., Wonka, P., Tulyakov, S.: 3davatargan: Bridging domains for personalized editable avatars. In: CVPR (2023) 6, 15
* (2) Afouras, T., Chung, J.S., Senior, A., Vinyals, O., Zisserman, A.: Deep audio-visual speech recognition. TPAMI (2018) 5, 11
* (3) Afouras, T., Chung, J.S., Zisserman, A.: Lrs3-ted: a large-scale dataset for visual speech recognition. arXiv (2018) 5
* (4) Agarwal, M., Mukhopadhyay, R., Namboodiri, V.P., Jawahar, C.: Audio-visual face reenactment. In: WACV (2023) 17
* (5) Agarwal, S., Farid, H.: Detecting deep-fake videos from aural and oral dynamics. In: CVPR (2021) 14
* (6) Ancilotto, A., Paissan, F., Farella, E.: Ximswap: many-to-many face swapping for tinyml. TECS (2023) 1, 8
* (7) Aneja, S., Thies, J., Dai, A., Niessner, M.: Clipface: Text-guided editing of textured 3d morphable models. In: SIGGRAPH (2023) 12
* (8) Ba, Z., Liu, Q., Liu, Z., Wu, S., Lin, F., Lu, L., Ren, K.: Exposing the deception: Uncovering more forgery clues for deepfake detection. In: AAAI (2024) 13, 14
* (9) Bao, J., Chen, D., Wen, F., Li, H., Hua, G.: Cvae-gan: fine-grained image generation through asymmetric training. In: ICCV (2017) 4, 8
* (10) Bao, J., Chen, D., Wen, F., Li, H., Hua, G.: Towards open-set identity preserving face synthesis. In: CVPR (2018) 7, 8
* (11) Barattin, S., Tzelepis, C., Patras, I., Sebe, N.: Attribute-preserving face dataset anonymization via latent code optimization. In: CVPR (2023) 1, 7
* (12) Binkowski, M., Sutherland, D.J., Arbel, M., Gretton, A.: Demystifying mmd gans. In: ICLR (2018) 5, 16
* (13) Bitouk, D., Kumar, N., Dhillon, S., Belhumeur, P., Nayar, S.K.: Face swapping: automatically replacing faces in photographs. In: SIGGRAPH (2008) 7
* (14) Blanz, V., Scherbaum, K., Vetter, T., Seidel, H.P.: Exchanging faces in images. In: Eurographics (2004) 7
* (15) Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al.: Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv (2023) 1, 2, 4, 9* (16) Bounareli, S., Tzelepis, C., Argyriou, V., Patras, I.: One-shot neural face reenactment via finding directions in gan's latent space. IJCV (2024) 9, 10
* (17) Bounareli, S., Tzelepis, C., Argyriou, V., Patras, I., Tzimiropoulos, G.: Hyperreenact: one-shot reenactment via jointly learning to refine and retarget faces. In: ICCV (2023) 1, 9, 10
* (18) Bounareli, S., Tzelepis, C., Argyriou, V., Patras, I., Tzimiropoulos, G.: Stytemask: Disentangling the style space of stylegan2 for neural face reenactment. In: FG (2023) 9, 10
* (19) Burgos-Artizzu, X.P., Perona, P., Dollar, P.: Robust face landmark estimation under occlusion. In: ICCV (2013) 7
* (20) Cai, Q., Ma, M., Wang, C., Li, H.: Image neural style transfer: A review. Computers and Electrical Engineering (2023) 6
* (21) Cao, J., Ma, C., Yao, T., Chen, S., Ding, S., Yang, X.: End-to-end reconstruction-classification learning for face forgery detection. In: CVPR (2022) 13, 14, 18
* (22) Cao, Q., Shen, L., Xie, W., Parkhi, O.M., Zisserman, A.: Vggface2: A dataset for recognising faces across pose and age. In: FG (2018) 5
* (23) Cao, W., Wang, T., Dong, A., Shu, M.: Transfs: Face swapping using transformer. In: FG (2023) 7, 9
* (24) Chai, L., Bau, D., Lim, S.N., Isola, P.: What makes fake images detectable? understanding properties that generalize. In: ECCV (2020) 13
* (25) Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo, O., Guibas, L.J., Tremblay, J., Khamis, S., et al.: Efficient geometry-aware 3d generative adversarial networks. In: CVPR (2022) 11, 15
* (26) Chang, H., Lu, J., Yu, F., Finkelstein, A.: Pairedcyclegan: Asymmetric style transfer for applying and removing makeup. In: CVPR (2018) 6, 16
* (27) Chaudhuri, B., Vesdapunt, N., Shapiro, L., Wang, B.: Personalized face modeling for improved face reconstruction and motion retargeting. In: ECCV (2020) 6
* (28) Chaudhuri, B., Vesdapunt, N., Wang, B.: Joint face detection and facial motion retargeting for multiple faces. In: CVPR (2019) 5
* (29) Chen, B., Hu, S., Chen, Q., Du, C., Yi, R., Qian, Y., Chen, X.: Gstalker: Real-time audio-driven talking face generation via deformable gaussian splatting. arXiv (2024) 11
* (30) Chen, C., Gong, D., Wang, H., Li, Z., Wong, K.Y.K.: Learning spatial attention for face super-resolution. TIP (2020) 6
* (31) Chen, H.J., Hui, K.M., Wang, S.Y., Tsao, L.W., Shuai, H.H., Cheng, W.H.: Beautyglow: On-demand makeup transfer framework with reversible generative network. In: CVPR (2019) 16
* (32) Chen, L., Li, Z., Maddox, R.K., Duan, Z., Xu, C.: Lip movements generation at a glance. In: ECCV (2018) 5, 10, 11, 16
* (33) Chen, L., Maddox, R.K., Duan, Z., Xu, C.: Hierarchical cross-modal talking face generation with dynamic pixel-wise loss. In: CVPR (2019) 6, 10, 11, 16
* (34) Chen, R., Chen, X., Ni, B., Ge, Y.: Simswap: An efficient framework for high fidelity face swapping. In: ACM MM (2020) 7, 8, 17
* (35) Chen, X., Dong, C., Ji, J., Cao, J., Li, X.: Image manipulation detection by multi-view multi-scale supervision. In: ICCV (2021) 5
* (36) Chen, Z., Lin, J., Zhou, T., Wu, F.: Sequential gating ensemble network for noise robust multiscale face restoration. IEEE transactions on cybernetics (2019) 6, 15
* (37) Cho, K., Lee, J., Yoon, H., Hong, Y., Ko, J., Ahn, S., Kim, S.: Gaussiantalker: Real-time high-fidelity talking head synthesis with audio-driven 3d gaussian splatting. arXiv (2024) 11
* (38) Choi, J., Kim, T., Jeong, Y., Baek, S., Choi, J.: Exploiting style latent flows for generalizing deepfake detection. In: CVPR (2024) 13, 14
* (39) Chung, J.S., Nagrani, A., Zisserman, A.: Voxceleb2: Deep speaker recognition. In: Interspeech (2018) 5
* (40) Cooke, M., Barker, J., Cunningham, S., Shao, X.: An audio-visual corpus for speech perception and automatic speech recognition. JASA (2006) 11
* (41) Cozzolino, D., Pianese, A., Niessner, M., Verdoliva, L.: Audio-visual person-of-interest deepfake detection. In: CVPR (2023) 13
* (42) Cui, K., Wu, R., Zhan, F., Lu, S.: Face transformer: Towards high fidelity and accurate face swapping. In: CVPR (2023) 7, 9
* (43) Dale, K., Sunkavalli, K., Johnson, M.K., Vlasic, D., Matusik, W., Pfister, H.: Video face replacement. In: SIGGRAPH (2011) 7
* (44) Dalva, Y., Altundis, S.F., Dundar, A.: Vecgan: Image-to-image translation with interpretable latent directions. In: ECCV (2022) 12
* (45) Dang, H., Liu, F., Stehouwer, J., Liu, X., Jain, A.K.: On the detection of digital face manipulation. In: CVPR (2020) 14, 15
* (46) Deng, J., Guo, J., Xue, N., Zafeiriou, S.: Arcface: Additive angular margin loss for deep face recognition. In: CVPR (2019) 16
* (47) Deng, Y., Yang, J., Xu, S., Chen, D., Jia, Y., Tong, X.: Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In: CVPR (2019) 16
* (48) DFD: Dfd. [https://blog.research.google/2019/09/contributing-data-to-deepfake-detection.html](https://blog.research.google/2019/09/contributing-data-to-deepfake-detection.html) (2019) 5
* (49) Ding, Z., Zhang, X., Xia, Z., Jebe, L., Tu, Z., Zhang, X.: Diffusionrig: Learning personalized priors for facial appearance editing. In: CVPR (2023) 12, 13
* (50) Dolhansky, B., Bitton, J., Pflaum, B., Lu, J., Howes, R., Wang, M., Ferrer, C.C.: The deepfake detection challenge (dfdc) dataset. arXiv (2020) 5, 18
* (51) Dolhansky, B., Howes, R., Pflaum, B., Baram, N., Ferrer, C.C.: The deepfake detection challenge (dfdc) preview dataset. arXiv (2019) 5
* (52) Dong, J., Wang, W., Tan, T.: Casia image tampering detection evaluation database. In: ChinaSIP (2013) 14
* (53) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021) 4
* (54) Dou, P., Shah, S.K., Kakadiaris, I.A.: End-to-end 3d face reconstruction with deep neural networks. In: CVPR (2017) 6
* (55) Doukas, M.C., Ververas, E., Sharmanska, V., Zafeiriou, S.: Free-headgan: Neural talking head synthesis with explicit gaze control. TPAMI (2023) 9, 16, 17
* (56) Doukas, M.C., Zafeiriou, S., Sharmanska, V.: Headgan: One-shot neural head synthesis and editing. In: ICCV (2021) 9
* (57) Du, C., Chen, Q., He, T., Tan, X., Chen, X., Yu, K., Zhao, S., Bian, J.: Dae-talker: High fidelity speech-driven talking face generation with diffusion autoencoder. In: ACM MM (2023) 11* [58] Fan, B., Wang, L., Soong, F.K., Xie, L.: Photo-real talking head with deep bidirectional lstm. In: ICASSP (2015) 6, 10
* [59] Feng, C., Chen, Z., Owens, A.: Self-supervised video forensics by audio-visual anomaly detection. In: CVPR (2023) 13
* [60] Feng, J., Singhal, P.: 3d face style transfer with a hybrid solution of nerf and mesh rasterization. In: WACV (2024) 6
* [61] Feng, M., Gilani, S.Z., Wang, Y., Mian, A.: 3d face reconstruction from light field images: A model-free approach. In: ECCV (2018) 6
* [62] Frank, J., Eisenhofer, T., Schonherr, L., Fischer, A., Kolossa, D., Holz, T.: Leveraging frequency analysis for deep fake image recognition. In: ICML (2020) 14
* [63] Fu, H., Wang, Z., Gong, K., Wang, K., Chen, T., Li, H., Zeng, H., Kang, W.: Mimic: Speaking style disentanglement for speech-driven 3d facial animation. In: AAAI (2024) 10
* [64] Gan, Y., Yang, Z., Yue, X., Sun, L., Yang, Y.: Efficient emotional adaptation for audio-driven talking-head generation. In: ICCV (2023) 10, 11
* [65] Gao, G., Huang, H., Fu, C., Li, Z., He, R.: Information bottleneck disentanglement for identity swapping. In: CVPR (2021) 8
* [66] Gao, K., Gao, Y., He, H., Lu, D., Xu, L., Li, J.: Nerf: Neural radiance field in 3d vision, a comprehensive review. arXiv (2022) 2, 5
* [67] Gao, Y., Wei, F., Bao, J., Gu, S., Chen, D., Wen, F., Lian, Z.: High-fidelity and arbitrary face editing. In: CVPR (2021) 12, 18
* [68] Gao, Y., Zhou, Y., Wang, J., Li, X., Ming, X., Lu, Y.: High-fidelity and freely controllable talking head video generation. In: CVPR (2023) 9, 17
* [69] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: NeurIPS (2014) 2, 4, 5
* [70] Gross, R., Matthews, I., Cohn, J., Kanade, T., Baker, S.: Multi-pie. IVC (2010) 7
* [71] Gu, Q., Wang, G., Chiu, M.T., Tai, Y.W., Tang, C.K.: Ladn: Local adversarial disentangling network for facial makeup and de-makeup. In: ICCV (2019) 6, 16
* [72] Gu, Z., Chen, Y., Yao, T., Ding, S., Li, J., Ma, L.: Delving into the local: Dynamic inconsistency learning for deepfake video detection. In: AAAI (2022) 13, 14
* [73] Guo, D., Sim, T.: Digital face makeup by example. In: CVPR (2009) 15
* [74] Guo, H., Niu, D., Kong, X., Zhao, X.: Face replacement based on 2d dense mapping. In: ICIGP (2019) 7
* [75] Guo, X., Liu, X., Ren, Z., Grosz, S., Masi, I., Liu, X.: Hierarchical fine-grained image forgery detection and localization. In: CVPR (2023) 14, 15
* [76] Guo, Y., Chen, K., Liang, S., Liu, Y.J., Bao, H., Zhang, J.: Ad-nerf: Audio driven neural radiance fields for talking head synthesis. In: ICCV (2021) 11
* [77] Guo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D., Dai, B.: Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In: ICLR (2024) 1, 2
* [78] Guo, Y., Zhang, L., Hu, Y., He, X., Gao, J.: Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In: ECCV (2016) 7
* [79] Guo, Y., Zhen, C., Yan, P.: Controllable guide-space for generalizable face forgery detection. In: ICCV (2023) 15, 18
* [80] Guo, Z., Jia, Z., Wang, L., Wang, D., Yang, G., Kasabov, N.: Constructing new backbone networks via space-frequency interactive convolution for deepfake detection. TIFS (2023) 14
* [81] Gupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Fei-Fei, L., Essa, I., Jiang, L., Lezama, J.: Photorealistic video generation with diffusion models. arXiv (2023) 4
* [82] Gururani, S., Mallya, A., Wang, T.C., Valle, R., Liu, M.Y.: Space: Speech-driven portrait animation with controllable expression. In: ICCV (2023) 10, 11, 18
* [83] Ha, S., Kersner, M., Kim, B., Seo, S., Kim, D.: Marionette: Few-shot face reenactment preserving identity of unseen targets. In: AAAI (2020) 9, 16
* [84] Haliassos, A., Mira, R., Petridis, S., Pantic, M.: Leveraging real talking faces via self-supervision for robust forgery detection. In: CVPR (2022) 2, 13, 18
* [85] Haliassos, A., Vougioulas, K., Petridis, S., Pantic, M.: Lips don't lie: A generalisable and robust approach to face forgery detection. In: CVPR (2021) 14, 18
* [86] Han, S., Lin, C., Shen, C., Wang, Q., Guan, X.: Interpreting adversarial examples in deep learning: A review. CSUR (2023) 6
* [87] Han, Y., Zhang, J., Zhu, J., Li, X., Ge, Y., Li, W., Wang, C., Liu, Y., Liu, X., Tai, Y.: A generalist facex via learning unified facial representation. arXiv (2023) 6, 7, 8
* [88] Hao, M., Gu, G., Fu, H., Liu, C., Cui, D.: Cumtgan: An instance-level controllable u-net gan for facial makeup transfer. Knowledge-Based Systems (2022) 6, 16
* [89] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016) 4
* [90] He, P., Li, H., Wang, H.: Detection of fake images via the ensemble of deep representations from multi color spaces. In: ICIP (2019) 2, 13
* [91] He, Z., Zuo, W., Kan, M., Shan, S., Chen, X.: Attgan: Facial attribute editing by only changing what you want. TIP (2019) 12
* [92] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. In: NeurIPS (2017) 5, 16
* [93] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS (2020) 2, 4, 5
* [94] Hou, X., Zhang, X., Li, Y., Shen, L.: Textface: Text-to-style mapping based face generation and manipulation. TMM (2022) 21, 18
* [95] Hou, X., Zhang, X., Liang, H., Shen, L., Lai, Z., Wan, J.: Guidedstyle: Attribute knowledge guided style manipulation for semantic face editing. Neural Networks (2022) 12, 18
* [96] Hsu, C.C., Lee, C.Y., Zhuang, Y.X.: Learning to detect fake face images in the wild. In: IS3C (2018) 15
* [97] Hsu, G.S., Tsai, C.H., Wu, H.Y.: Dual-generator face reenactment. In: CVPR (2022) 1, 9, 17
* [98] Hsu, Y.F., Chang, S.F.: Detecting image splicing using geometry invariants and camera characteristics consistency. In: ICME (2006) 14
* [99] Hu, J., Liao, X., Liang, J., Zhou, W., Qin, Z.: Finfer: Frame inference-based deepfake detection for high-visual-quality videos. In: AAAI (2022) 14, 15
* [100] Hu, L., Gao, X., Zhang, P., Sun, K., Zhang, B., Bo, L.: Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv (2023) 15
* [101] Hu, X., Ma, P., Mai, Z., Peng, S., Yang, Z., Wang, L.: Face hallucination from low quality images using definition-scalable inference. PR (2019) 15* [102] Huang, B., Wang, Z., Yang, J., Ai, J., Zou, Q., Wang, Q., Ye, D.: Implicit identity driven deepfake face swapping detection. In: CVPR (2023) 5, 14, 15
* [103] Huang, G.B., Mattar, M., Berg, T., Learned-Miller, E.: Labeled faces in the wild: A database forstudying face recognition in unconstrained environments. In: Workshop on faces in'Real-Life'Images: detection, alignment, and recognition (2008) 5
* [104] Huang, H., He, R., Sun, Z., Tan, T.: Wavelet-srnet: A wavelet-based cnn for multi-scale face super resolution. In: ICCV (2017) 6, 15
* [105] Huang, W., Tu, S., Xu, L.: Ia-faces: A bidirectional method for semantic face editing. Neural Networks (2023) 2, 12, 18
* [106] Huang, Z., Chan, K.C., Jiang, Y., Liu, Z.: Collaborative diffusion for multi-modal face generation and editing. In: CVPR (2023) 12
* [107] Hur, J., Choi, J., Han, G., Lee, D.J., Kim, J.: Expanding expressiveness of diffusion models with limited data via self-distillation based fine-tuning. In: WACV (2024) 6, 15
* [108] Hwang, S., Hyung, J., Kim, D., Kim, M.J., Choo, J.: Facelipenf: Text-driven 3d face manipulation using deformable neural radiance fields. In: ICCV (2023) 5
* [109] Ilyas, H., Javed, A., Malik, K.M.: Avfakenet: A unified end-to-end dense swin transformer deep learning model for audio-visual deepfakes detection. Applied Soft Computing (2023) 2
* [110] Irshad, M.Z., Zakharov, S., Liu, K., Guizilini, V., Kollar, T., Gaidon, A., Kira, Z., Ambrus, R.: Neo 360: Neural fields for sparse view synthesis of outdoor scenes. In: ICCV (2023) 5
* [111] Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with conditional adversarial networks. In: ICCV (2017) 4
* [112] Jain, R., Singh, K.K., Hemani, M., Lu, J., Sarkar, M., Ceylan, D., Krishnamurthy, B.: Vgflow: Visibility guided flow network for human reposing. In: CVPR (2023) 15
* [113] Ji, X., Zhou, H., Wang, K., Wu, W., Loy, C.C., Cao, X., Xu, F.: Audio-driven emotional video portraits. In: CVPR (2021) 11
* [114] Jiang, D., Song, D., Tong, R., Tang, M.: Styleipsb: Identity-preserving semantic basis of stylegan for high fidelity face swapping. In: CVPR (2023) 8, 17
* [115] Jiang, K., Chen, S.Y., Liu, F.L., Fu, H., Gao, L.: Nerf-faceediting: Disentangled face editing in neural radiance fields. In: SIGGRAPH (2022) 12
* [116] Jiang, K., Wang, Z., Yi, P., Wang, G., Gu, K., Jiang, J.: Atmfr: Adaptive-threshold-based multi-model fusion network for compressed face hallucination. TMM (2019) 6
* [117] Jiang, L., Li, R., Wu, W., Qian, C., Loy, C.C.: Deeperforensics-1.0: A large-scale dataset for real-world face forgery detection. In: CVPR (2020) 5, 18
* [118] Jiang, L., Zhang, J., Deng, B., Li, H., Liu, L.: 3d face reconstruction with geometry details from a single image. TIP (2018) 6
* [119] Jiang, W., Liu, S., Gao, C., Cao, J., He, R., Feng, J., Yan, S.: Pgsan: Pose and expression robust spatial-aware gan for customizable makeup transfer. In: CVPR (2020) 6, 16
* [120] Jiang, Y., Jiang, L., Yang, S., Loy, C.C.: Scenimefy: Learning to craft anime scene via semi-supervised image-to-image translation. In: ICCV (2023) 6, 15
* [121] Jo, Y., Park, J.: Sc-fegan: Face editing generative adversarial network with user's sketch and color. In: ICCV (2019) 12
* [122] Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: ECCV (2016) 5
* [123] Karras, J., Holynski, A., Wang, T.C., Kemelmacher-Shlizerman, I.: Dreampose: Fashion image-to-video synthesis via stable diffusion. In: ICCV (2023) 11
* [124] Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for improved quality, stability, and variation. In: ICLR (2018) 5
* [125] Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: CVPR (2019) 1, 2, 4, 5, 12
* [126] Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of stylegan. In: CVPR (2020) 1
* [127] Kellnhofer, P., Recasens, A., Stent, S., Matusik, W., Torralba, A.: Gaze360: Physically unconstrained gaze estimation in the wild. In: ICCV (2019) 9, 17
* [128] Kemelmacher-Shlizerman, I., Basri, R.: 3d face reconstruction from a single image using a single reference face shape. TPAMI (2010) 6
* [129] Kerbl, B., Kopanas, G., Leimkuhler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM TOG (2023) 11
* [130] Khalid, H., Tariq, S., Kim, M., Woo, S.S.: Fakeaveeleb: A novel audio-video multimodal deepfake dataset. NeurIPS (2021) 5, 14
* [131] Kim, G., Shim, H., Kim, H., Choi, Y., Kim, J., Yang, E.: Diffusion video autoencoders: Toward temporally consistent face video editing via disentangled video encoding. In: CVPR (2023) 12, 18
* [132] Kim, H., Elgharib, M., Zollhofer, M., Seidel, H.P., Beeler, T., Richardt, C., Theobalt, C.: Neural style-preserving visual dubbing. ACM TOG (2019) 9
* [133] Kim, H., Garrido, P., Tewari, A., Xu, W., Thies, J., Niessner, M., Perez, P., Richardt, C., Zollhofer, M., Theobalt, C.: Deep video portraits. ACM TOG (2018) 9
* [134] Kim, J., Lee, J., Zhang, B.T.: Smooth-swap: a simple enhancement for face-swapping with smoothness. In: CVPR (2022) 7, 8
* [135] Kim, K., Kim, Y., Cho, S., Seo, J., Nam, J., Lee, K., Kim, S., Lee, K.: Diffuse: Diffusion-based face swapping with facial guidance. arXiv (2022) 7, 8
* [136] Kingma, D.P., Welling, M.: Auto-encoding variational bayes. stat (2014) 1, 4, 5
* [137] Ko, S., Dai, B.R.: Multi-laplacian gan with edge enhancement for face super resolution. In: ICPR (2021) 6
* [138] Korshunov, P., Marcel, S.: Deepfakes: a new threat to face recognition? assessment and detection. arXiv (2018) 5, 14
* [139] Korshunova, I., Shi, W., Dambre, J., Theis, L.: Fast face-swap using convolutional neural networks. In: ICCV (2017) 9
* [140] Koujan, M.R., Doukas, M.C., Roussos, A., Zafeiriou, S.: Head2head: Video-based neural head synthesis. In: FG (2020) 1, 9
* [141] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NeurIPS (2012) 4, 16
* [142] Kwon, P., You, J., Nam, G., Park, S., Chae, G.: Kodf: A large-scale korean deepfake detection dataset. In: ICCV (2021) 5
* [143] Le, B.M., Woo, S.S.: Quality-agnostic deepfake detection with intra-model collaborative learning. In: ICCV (2023)* [144] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE (1998)
* [145] Lee, D., Kim, C., Yu, S., Yoo, J., Park, G.M.: Radio: Reference-agnostic dubbing video synthesis. In: WACV (2024)
* [146] Lei, B., Yu, K., Feng, M., Cui, M., Xie, X.: Diffusion-gan3d: Boosting text-guided 3d generation and domain adaption by combining 3d gans and diffusion priors. arXiv (2023)
* [147] Li, C., Zhou, K., Lin, S.: Simulating makeup through physics-based manipulation of intrinsic image layers. In: CVPR (2015)
* [148] Li, D., Zhao, K., Wang, W., Peng, B., Zhang, Y., Dong, J., Tan, T.: a-nerf: Audio enhanced neural radiance field for few shot talking head synthesis. In: AAAI (2024)
* [149] Li, J., Li, Z., Cao, J., Song, X., He, R.: Faceinpainter: High fidelity face adaptation to heterogeneous domains. In: CVPR (2021)
* [150] Li, J., Xie, H., Li, J., Wang, Z., Zhang, Y.: Frequency-aware discriminative feature learning supervised by single-center loss for face forgery detection. In: CVPR (2021)
* [151] Li, J., Zhang, J., Bai, X., Zheng, J., Ning, X., Zhou, J., Gu, L.: Talkinggaussian: Structure-persistent 3d talking head synthesis via gaussian splatting. arXiv (2024)
* [152] Li, L., Bao, J., Yang, H., Chen, D., Wen, F.: Faceshifter: Towards high fidelity and occlusion aware face swapping. CVPR (2020)
* [153] Li, L., Bao, J., Zhang, T., Yang, H., Chen, D., Wen, F., Guo, B.: Face x-ray for more general face forgery detection. In: CVPR (2020)
* [154] Li, M., Yu, W., Liu, Q., Li, Z., Li, R., Zhong, B., Zhang, S.: Hybrid transformers with attention-guided spatial embeddings for makeup transfer and removal. TCSVT (2023)
* [155] Li, W., Zhang, L., Wang, D., Zhao, B., Wang, Z., Chen, M., Zhang, B., Wang, Z., Bo, L., Li, X.: One-shot high-fidelity talking-head synthesis with deformable neural radiance field. In: CVPR (2023)
* [156] Li, Y., Chang, M.C., Lyu, S.: In ictu oculi: Exposing ai generated fake face videos by detecting eye blinking. In: WIFS (2018)
* [157] Li, Y., Ma, C., Yan, Y., Zhu, W., Yang, X.: 3d-aware face swapping. In: CVPR (2023)
* [158] Li, Y., Yang, X., Sun, P., Qi, H., Lyu, S.: Celeb-df: A large-scale challenging dataset for deepfake forensics. In: CVPR (2020)
* [159] Liang, B., Pan, Y., Guo, Z., Zhou, H., Hong, Z., Han, X., Han, J., Liu, J., Ding, E., Wang, J.: Expressive talking head generation with granular audio-visual control. In: CVPR (2022)
* [160] Lin, J., Yuan, Y., Shao, T., Zhou, K.: Towards high-fidelity 3d face reconstruction from in-the-wild images using graph convolutional networks. In: CVPR (2020)
* [161] Lin, Y., Wang, S., Lin, Q., Tang, F.: Face swapping under large pose variations: A 3d model based approach. In: ICME (2012)
* [162] Liu, F., Yu, L., Xie, H., Liu, C., Ding, Z., Yang, Q., Zhang, Y.: High fidelity face swapping via semantics disentanglement and structure enhancement. In: ACM MM (2023)
* [163] Liu, J., Huang, H., Jin, C., He, R.: Portrait diffusion: Training-free face stylization with chain-of-painting. arXiv (2023)
* [164] Liu, J., Wang, Q., Fan, H., Wang, Y., Tang, Y., Qu, L.: Residual denoising diffusion models. In: CVPR (2024)
* [165] Liu, K., Perov, I., Gao, D., Chervouniy, N., Zhou, W., Zhang, W.: Deepfacela: Integrated, flexible and extensible face-swapping framework. PR (2023)
* [166] Liu, R., Ma, B., Zhang, W., Hu, Z., Fan, C., Lv, T., Ding, Y., Cheng, X.: Towards a simultaneous and granular identity-expression control in personalized face generation. arXiv (2024)
* [167] Liu, S., Jiang, W., Gao, C., He, R., Feng, J., Li, B., Yan, S.: Psgam++: robust detail-preserving makeup transfer and removal. TPAMI (2021)
* [168] Liu, Y., Hu, B., Huang, J., Tai, Y.W., Tang, C.K.: Instance neural radiance field. In: ICCV (2023)
* [169] Liu, Y., Li, Q., Deng, Q., Sun, Z., Yang, M.H.: Gan-based facial attribute manipulation. TPAMI (2023)
* [170] Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L., et al.: Swin transformer v2: Scaling up capacity and resolution. In: CVPR (2022)
* [171] Liu, Z., Li, M., Zhang, Y., Wang, C., Zhang, Q., Wang, J., Nie, Y.: Fine-grained face swapping via regional gan inversion. In: CVPR (2023)
* [172] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)
* [173] Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In: ICCV (2015)
* [174] Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: CVPR (2022)
* [175] Liu, Z., Qi, X., Torr, P.H.: Global texture enhancement for fake face detection in the wild. In: CVPR (2020)
* [176] Lu, T., Wang, J., Jiang, J., Zhang, Y.: Global-local fusion network for face super-resolution. Neurocomputing (2020)
* [177] Ma, Y., He, Y., Cun, X., Wang, X., Shan, Y., Li, X., Chen, Q.: Follow your pose: Pose-guided text-to-video generation using pose-free videos. arXiv (2023)
* [178] Ma, Y., Wang, S., Ding, Y., Ma, B., Lv, T., Fan, C., Hu, Z., Deng, Z., Yu, X.: Talkclip: Talking head generation with text-guided expressive speaking styles. arXiv (2023)
* [179] Ma, Y., Zhang, S., Wang, J., Wang, X., Zhang, Y., Deng, Z.: Dreamtalk: When expressive talking head generation meets diffusion probabilistic models. arXiv (2023)
* [180] Maninchedda, F., Oswald, M.R., Pollefeys, M.: Fast 3d reconstruction of faces with glasses. In: CVPR (2017)
* [181] Masi, I., Killekar, A., Mascarenhas, R.M., Gurudatt, S.P., AbdAlmageed, W.: Two-branch recurrent network for isolating deepfakes in videos. In: ECCV (2020)
* [182] Maze, B., Adams, J., Duncan, J.A., Kalka, N., Miller, T., Otto, C., Jain, A.K., Niggel, W.T., Anderson, J., Cheney, J., et al.: Iarpa janus benchmark-c: Face dataset and protocol. In: ICB (2018)
* [183] McCloskey, S., Albright, M.: Detecting gan-generated imagery using saturation cues. In: ICIP (2019)
* [184] Melnik, A., Miasayedzemkau, M., Makaravets, D., Pirshtuk, D., Akbulut, E., Holzmann, D., Renusch, T., Reichert, G., Ritter, H.: Face generation and editing with stylegan: A survey. TPAMI (2024)
* [185] Miao, C., Tan, Z., Chu, Q., Yu, N., Guo, G.: Hierarchical frequency-assisted interactive networks for face manipulation detection. TIFS (2022)* (186) Milborrow, S., Morkel, J., Nicolls, F.: The muct landmarked face database. PRASA (2010) 7
* (187) Mildenhall, B., Srinivasan, P., Tancik, M., Barron, J., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020) 2, 5, 12
* (188) Min, Z., Zhuang, B., Schulter, S., Liu, B., Dunn, E., Chandrase, M.: Neuros: Neural nocs supervision for monocular 3d object localization. In: CVPR (2023) 5
* (189) Mir, A., Alonso, E., Mondragon, E.: Dit-head: High-resolution talking head synthesis using diffusion transformers. In: ICAART (2024) 1, 6, 11
* (190) Mirsky, Y., Lee, W.: The creation and detection of deep-fakes: A survey. CSUR (2021) 2
* (191) Mirza, M., Osindero, S.: Conditional generative adversarial nets. arXiv (2014) 4
* (192) Moniz, J.R.A., Beckham, C., Rajotte, S., Honari, S., Pal, C.: Unsupervised depth estimation, 3d face rotation and replacement. In: NeurIPS (2018) 8
* (193) Moore, S., Bowden, R.: Multi-view pose and facial expression recognition. In: BMVC (2010) 5, 9
* (194) Morales, A., Piella, G., Sukno, F.M.: Survey on 3d face reconstruction from uncalibrated images. Computer Science Review (2021) 6
* (195) Mosaddegh, S., Simon, L., Jurie, F.: Photorealistic face de-identification by aggregating donors' face components. In: ACCV (2015) 7
* (196) Moser, B.B., Ruate, F., Frolov, S., Palacio, S., Hees, J., Dengel, A.: Hitchhiker's guide to super-resolution: Introduction and recent advances. TPAMI (2023) 6
* (197) Moser, B.B., Shanbhag, A.S., Raue, F., Frolov, S., Palacio, S., Dengel, A.: Diffusion models, image super-resolution and everything: A survey. In: AAAI (2024) 6
* (198) Nagrani, A., Chung, J.S., Zisserman, A.: Voxceleb: a large-scale speaker identification dataset. In: Interspeech (2017) 5, 17
* (199) Narayan, K., Agarwal, H., Thakral, K., Mittal, S., Vatsa, M., Singh, R.: Deephy: On deepfake phylogeny. In: IJCB (2022) 5
* (200) Narayan, K., Agarwal, H., Thakral, K., Mittal, S., Vatsa, M., Singh, R.: Df-platter: multi-face heterogeneous deepfake dataset. In: CVPR (2023) 5
* (201) Natsume, R., Yatagawa: Rgsan: face swapping and editing using face and hair representation in latent spaces. In: SIGGRAPH (2018) 6, 7, 8
* (202) Nguyen, H.H., Yamagishi, J., Echizen, I.: Capsuleforensics: Using capsule networks to detect forged images and videos. In: ICASSP (2019) 13
* (203) Nirkin, Y., Keller, Y., Hassner, T.: Fsgan: Subject agnostic face swapping and reenactment. In: ICCV (2019) 7, 8
* (204) Nirkin, Y., Keller, Y., Hassner, T.: Fsganv2: Improved subject agnostic face swapping and reenactment. TPAMI (2022) 7
* (205) Nirkin, Y., Masi, I., Tuan, A.T., Hassner, T., Medioni, G.: On face segmentation, face swapping, and face perception. In: FG (2018) 7
* (206) Nirkin, Y., Wolf, L., Keller, Y., Hassner, T.: Deepfake detection based on discrepancies between faces and their context. TPAMI (2021) 13, 14
* (207) Oordiff, T., Yacoob, Y.: Robust one-shot face video reenactment using hybrid latent spaces of stylegan2. In: ICCV (2023) 9, 10
* (208) Ozkan, S., Ozay, M., Robinson, T.: Conceptual and hierarchical latent space decomposition for face editing. In: ICCV (2023) 12
* (209) Pan, Y., Tang, J., Tjahjadi, T.: Lpsrgan: Generative adversarial networks for super-resolution of license plate image. Neurocomputing (2024) 15
* (210) Pang, Y., Zhang, Y., Quan, W., Fan, Y., Cun, X., Shan, Y., Yan, D.m.: Dpe: Disentanglement of pose and expression for general video portrait editing. In: CVPR (2023) 2
* (211) Parkhi, O., Vedaldi, A., Zisserman, A.: Deep face recognition. In: BMVC (2015) 5
* (212) Peng, C., Miao, Z., Liu, D., Wang, N., Hu, R., Gao, X.: Where deepfakes gaze at? spatial-temporal gaze inconsistency analysis for video face forgery detection. TIFS (2024) 13, 14
* (213) Peng, X., Zhu, J., Jiang, B., Tai, Y., Luo, D., Zhang, J., Lin, W., Jin, T., Wang, C., Ji, R.: Portraitbooth: A versatile portrait model for fast identity-preserved personalization. In: CVPR (2024) 6
* (214) Peng, Z., Hu, W., Shi, Y., Zhu, X., Zhang, X., Zhao, H., He, J., Liu, H., Fan, Z.: Synctalk: The devil is in the synchronization for talking head synthesis. In: CVPR'24 (2024) 11
* (215) Peng, Z., Wu, H., Song, Z., Xu, H., Zhu, X., He, J., Liu, H., Fan, Z.: Emotalk: Speech-driven emotional disentanglement for 3d face animation. In: ICCV (2023) 11
* (216) Perez, J.C., Nguyen-Phuoc, T., Cao, C., Sanakoyeu, A., Simon, T., Arbelaez, P., Ghanem, B., Thabet, A., Pumarola, A.: Styleavatar: Stylizing animatable head avatars. In: WACV (2024) 6
* (217) Prajwal, K., Mukhopadhyay, R., Namboodiri, V.P., Jawahar, C.: A lip sync expert is all you need for speech to lip generation in the wild. In: ACM MM (2020) 5, 10, 11
* (218) Preechakul, K., Chatthee, N., Wizadwongsa, S., Suwajanakorn, S.: Diffusion autoencoders: Toward a meaningful and decodable representation. In: CVPR (2022) 12, 18
* (219) Qian, Y., Yin, G., Sheng, L., Chen, Z., Shao, J.: Thinking in frequency: Face forgery detection by mining frequency-aware clues. In: ECCV (2020) 2, 14, 18
* (220) Ren, X., Chen, X., Yao, P., Shum, H.Y., Wang, B.: Reinforced disentanglement for face swapping without skip connection. In: ICCV (2023) 7, 8, 17
* (221) Richie, C., Warburton, S., Carter, M.: Audiovisual database of spoken american english. LDC (2009) 11
* (222) Rochow, A., Schwarz, M., Behnke, S.: Fsrt: Facial scene representation transformer for face reenactment from factorized appearance, head-pose, and facial expression features. In: CVPR (2024) 9
* (223) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR (2022) 2, 4
* (224) Rosberg, F., Aksoy, E.E., Alonso-Fernandez, F., Englund, C.: Facedancer: pose-and occlusion-aware high fidelity face swapping. In: WACV (2023) 8
* (225) Rossler, A., Cozzolino, D., Verdoliva, L., Riess, C., Thies, J., Niessner, M.: Faceforensics++: Learning to detect manipulated facial images. In: ICCV (2019) 5, 17, 18
* (226) Ruiz, N., Chong, E., Rehg, J.M.: Fine-grained head pose estimation without keypoints. In: CVPR (2018) 5, 16
* (227) Sha, T., Zhang, W., Shen, T., Li, Z., Mei, T.: Deep person generation: A survey from the perspective of face, pose, and cloth synthesis. CSUR (2023) 1, 2
* (228) Sharma, P., Tewari, A., Du, Y., Zakharov, S., Ambrus, R.A., Gaidon, A., Freeman, W.T., Durand, F., Tenenbaum, J.B., Sitzmann, V.: Neural groundplans: Persistent neural scene representations from a single image. In: ICLR (2023)* [229] Sharma, S., Kumar, V.: 3d face reconstruction in deep learning era: A survey. ACME (2022)
* [230] Shen, S., Li, W., Zhu, Z., Duan, Y., Zhou, J., Lu, J.: Learning dynamic facial radiance fields for few-shot talking head synthesis. In: ECCV (2022)
* [231] Shen, W., Liu, R.: Learning residual images for face attribute manipulation. In: CVPR (2017)
* [232] Shen, Y., Gu, J., Tang, X., Zhou, B.: Interpreting the latent space of gans for semantic face editing. In: CVPR (2020)
* [233] Sheng, Z., Nie, L., Zhang, M., Chang, X., Yan, Y.: Stochastic latent talking face generation towards emotional expressions and head poses. TCSVT (2023)
* [234] Shi, Y., Li, G., Cao, Q., Wang, K., Lin, L.: Face hallucination by attentive sequence optimization with reinforcement learning. TPAMI (2019)
* [235] Shiohara, K., Yamasaki, T.: Detecting deepfakes with self-blended images. In: CVPR (2022)
* [236] Shiohara, K., Yang, X., Taketomi, T.: Blendface: Re-designing identity encoders for face-swapping. In: ICCV (2023)
* [237] Shu, C., Wu, H., Zhou, H., Liu, J., Hong, Z., Ding, C., Han, J., Liu, J., Ding, E., Wang, J.: Few-shot head swapping in the wild. In: CVPR (2022)
* [238] Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsupervised learning using nonequilibrium thermodynamics. In: ICML (2015)
* [239] Sohn, K., Lee, H., Yan, X.: Learning structured output representation using deep conditional generative models. In: NeurIPS (2015)
* [240] Stypulkowski, M., Vougioukas, K., He, S., Zieba, M., Petridis, S., Pantic, M.: Diffused heads: Diffusion models beat gans on talking-face generation. In: WACV (2024)
* [241] Sun, J., Deng, Q., Li, Q., Sun, M., Ren, M., Sun, Z.: Anyface: Free-style text-to-face synthesis and manipulation. In: CVPR (2022)
* [242] Sun, J., Li, Q., Wang, W., Zhao, J., Sun, Z.: Multi-caption text-to-face synthesis: Dataset and algorithm. In: ACM MM (2021)
* [243] Sun, J., Wang, X., Zhang, Y., Li, X., Zhang, Q., Liu, Y., Wang, J.: Fener: Face editing in neural radiance fields. In: CVPR (2022)
* [244] Sun, Q., Tewari, A., Xu, W., Fritz, M., Theobalt, C., Schiele, B.: A hybrid model for identity obfuscation by face replacement. In: ECCV (2018)
* [245] Sun, Z., Chen, Y., Xiong, S.: Ssat: A symmetric semantic-aware transformer network for makeup transfer and removal. In: AAAI (2022)
* [246] Sun, Z., Chen, Y., Xiong, S.: Ssat ++: A semantic-aware and versatile makeup transfer network with local color consistency constraint. TNNLS (2023)
* [247] Sunkavalli, K., Johnson, M.K., Matusik, W., Pfister, H.: Multi-scale image harmonization. ACM TOG (2010)
* [248] Tan, C., Zhao, Y., Wei, S., Gu, G., Liu, P., Wei, Y.: Frequency-aware deepfake detection: Improving generalizability through frequency space domain learning. In: AAAI (2024)
* [249] Tan, C., Zhao, Y., Wei, S., Gu, G., Wei, Y.: Learning on gradients: Generalized artifacts representation for gans generated images detection. In: CVPR (2023)
* [250] Tan, S., Ji, B., Pan, Y.: Emmn: Emotional motion memory network for audio-driven emotional talking face generation. In: ICCV (2023)
* [251] Thies, J., Zollhofer, M., Stamminger, M., Theobalt, C., Niessner, M.: Face2face: Real-time face capture and reenactment of rgb videos. In: CVPR (2016)
* [252] Tian, L., Wang, Q., Zhang, B., Bo, L.: Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions. arXiv (2024)
* [253] Tripathy, S., Kannala, J., Rahtu, E.: Icface: Interpretable and controllable face reenactment using gans. In: WACV (2020)
* [254] Tulsiani, S., Zhou, T., Efros, A.A., Malik, J.: Multi-view supervision for single-view reconstruction via differentiable ray consistency. In: CVPR (2017)
* [255] Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning. In: NeurIPS (2017)
* [256] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)
* [257] Yyas, K., Pareek, P., Jaayswal, R., Patil, S.: Analysing the landscape of deep fake detection: A survey. IJISAE (2024)
* [258] Wang, H., Wang, Y., Zhou, Z., Ji, X., Gong, D., Zhou, J., Li, Z., Liu, W.: Cosface: Large margin cosine loss for deep face recognition. In: CVPR (2018)
* [259] Wang, J., Qian, X., Zhang, M., Tan, R.T., Li, H.: Seeing what you said: Talking face generation guided by a lip reading expert. In: CVPR (2023)
* [260] Wang, J., Wu, Z., Ouyang, W., Han, X., Chen, J., Jiang, Y.G., Li, S.N.: M2tr: Multi-modal multi-scale transformers for deepfake detection. In: ICML (2022)
* [261] Wang, J., Zhao, K., Zhang, S., Zhang, Y., Shen, Y., Zhao, D., Zhou, J.: Lipformer: High-fidelity and generalizable talking face generation with a pre-learned facial codebook. In: CVPR (2023)
* [262] Wang, J., Zhao, Y., Fan, H., Xu, T., Li, Q., Li, S., Liu, L.: Memory-augmented contrastive learning for talking head generation. In: ICASSP (2023)
* [263] Wang, K., Wu, Q., Song, L., Yang, Z., Wu, W., Qian, C., He, R., Qiao, Y., Loy, C.C.: Mead: A large-scale audio-visual dataset for emotional talking-face generation. In: ECCV (2020)
* [264] Wang, Q., Liu, L., Hua, M., Zhu, P., Zuo, W., Hu, Q., Lu, H., Cao, B.: Hs-diffusion: Semantic-mixing diffusion for head swapping. arXiv (2023)
* [265] Wang, R., Juefei-Xu, F., Ma, L., Xie, X., Huang, Y., Wang, J., Liu, Y.: Fakespotter: a simple yet robust baseline for spotting ai-synthesized fake faces. In: IJCAI (2021)
* [266] Wang, S., Ma, Y., Ding, Y., Hu, Z., Fan, C., Lv, T., Deng, Z., Yu, X.: Stytelalk++: A unified framework for controlling the speaking styles of talking heads. TPAMI (2024)
* [267] Wang, T., Chow, K.P.: Noise based deepfake detection via multi-head relative-interaction. In: AAAI (2023)
* [268] Wang, T., Li, L., Lin, K., Lin, C.C., Yang, Z., Zhang, H., Liu, Z., Wang, L.: Disco: Disentangled control for referring human dance generation in real world. In: CVPR (2024)
* [269] Wang, T., Li, Z., Liu, R., Wang, Y., Nie, L.: An efficient attribute-preserving framework for face swapping. TMM (2024)
* [270] Wang, T.C., Mallya, A., Liu, M.Y.: One-shot free-view neural talking-head synthesis for video conferencing. In: CVPR (2021)
* [271] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer:A versatile backbone for dense prediction without convolutions. In ICCV (2021) [72] Wang, Y., Chen, X., Zhu, J., Chu, W., Tai, Y., Wang, C., Li, J., Wu, Y., Huang, F., Ji, R.: Hiffiface: 3d shape and semantic prior guided high fidelity face swapping. In: IJCAI (2021) [73] Wang, Y., Ma, X., Chen, X., Dantcheva, A., Dai, B., Qiao, Y.: Leo: Generative latent image animator for human video synthesis. arXiv (2023) [6] [274] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. TIP (2004) [5] Wei, H., Yang, Z., Wang, Z.: Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv (2024) [276] Wen, B., Zhu, Y., Subramanian, R., Ng, T.T., Shen, X., Winkler, S.: Coverage--a novel database for copy-move forgery detection. In: ICIP (2016) [277] Wenmin Huang, W.L., Jiwu Huang, X.C.: Sdgan: Disentangling semantic manipulation for facial attribute editing. In: AAAI (2024) [278] Wiles, O., Koepke, A., Zisserman, A.: X2face: A network for controlling face generation using images, audio, and pose codes. In: ECCV (2018) [279] Wu, X., Hu, P., Wu, Y., Lyu, X., Cao, Y.P., Shan, Y., Yang, W., Sun, Z., Qi, X.: Speech2lip: High-fidelity speech to lip generation by learning from a short video. In: ICCV (2023) [280] Xia, W., Yang, Y., Xue, J.H., Wu, B.: Tedigan: Text-guided diverse face image generation and manipulation. In: CVPR (2021) [281] Xing, Y., Tewari, R., Mendonca, P.: A self-supervised bootstrap method for single-image 3d face reconstruction. In: WACV (2019) [282] Xu, C., Zhang, J., Han, Y., Tian, G., Zeng, X., Tai, Y., Wang, Y., Wang, C., Liu, Y.: Designing one unified framework for high-fidelity face reenactment and swapping. In: ECCV (2022) [283] Xu, C., Zhang, J., Hua, M., He, Q., Yi, Z., Liu, Y.: Region-aware face swapping. In: CVPR (2022) [284] Xu, C., Zhu, J., Zhang, J., Han, Y., Chu, W., Tai, Y., Wang, C., Xie, Z., Liu, Y.: High-fidelity generalized emotional talking face generation with multi-modal emotion space learning. In: CVPR (2023) [285] Xu, C., Zhu, S., Zhu, J., Huang, T., Zhang, J., Tai, Y., Liu, Y.: Multimodal-driven talking face generation via a unified diffusion-based generator. CoRR (2023) [286] Xu, J., Motamed, S., Vaddamanu, P., Wu, C.H., Haene, C., Bazin, J.C., De la Torre, F.: Personalized face inpainting with diffusion models by parallel visual attention. In: WACV (2024) [287] Xu, S., Chen, G., Guo, Y.X., Yang, J., Li, C., Zang, Z., Zhang, Y., Tong, X., Guo, B.: Vasa-1: Lifelike audio-driven talking faces generated in real time. arXiv (2024) [288] Xu, S., Li, L., Shen, L., Men, Y., Lian, Z.: Your3demoji: Creating personalized emojis via one-shot 3d-aware cartoon avatar synthesis. In: SIGGRAPH (2022) [289] Xu, Y., Deng, B., Wang, J., Jing, Y., Pan, J., He, S.: High-resolution face swapping via latent semantics disentanglement. In: CVPR (2022) [280] Xu, Y., Liang, J., Sheng, L., Zhang, X.Y.: Towards generalizable deepfake video detection with thumbnail layout and graph reasoning. IJCV (2024) [281] Xu, Y., Yin, Y., Jiang, L., Wu, Q., Zheng, C., Loy, C.C., Dai, B., Wu, W.: Transeditor: Transformer-based dual-space gan for highly controllable facial editing. In: CVPR (2022) [282] Xu, Z., Zhang, J., Liew, J.H., Yan, H., Liu, J.W., Zhang, C., Feng, J., Shou, M.Z.: Magicanimate: Temporally consistent human image animation using diffusion model. In: CVPR (2024) [283] Xu, Z., Zhou, H., Hong, Z., Liu, Z., Liu, J., Guo, Z., Han, J., Liu, J., Ding, E., Wang, J.: Styleswap: Style-based generator empowers robust face swapping. In: ECCV (2022) [7] [8] Xue, H., Ling, J., Tang, A., Song, L., Xie, R., Zhang, W.: High-fidelity face reenactment via identity-matched correspondence learning. TOMM (2023) [284] Yan, Q., Guo, C., Zhao, J., Dai, Y., Loy, C.C., Li, C.: Beautyrec: Robust, efficient, and component-specific makeup transfer. In: CVPR (2023) [6] Yang, C., He, W., Xu, Y., Gao, Y.: El elegant: Exquisite and locally editable gan for makeup transfer. In: ECCV (2022) [6] Yang, K., Chen, K., Guo, D., Zhang, S.H., Guo, Y.C., Zhang, W.: Face2face \(\rho\): Real-time high-resolution one-shot face reenactment. In: ECCV (2022) [6] Yang, P., Zhou, S., Tao, Q., Loy, C.C.: Pgdiff: Guiding diffusion models for versatile face restoration via partial guidance. In: NeurIPS (2024) [6] Yang, S., Jiang, L., Liu, Z., Loy, C.C.: Vtoonify: Controllable high-resolution portrait video style transfer. ACM TOG (2022) [6] Yang, S., Wang, W., Lan, Y., Fan, X., Peng, B., Yang, L., Dong, J.: Learning dense correspondence for nerf-based face reenactment. In: AAAI (2024) [6] Yang, W., Zhou, X., Chen, Z., Guo, B., Ba, Z., Xia, Z., Cao, X., Ren, K.: Avoid-df: Audio-visual joint learning for detecting deepfake. TIFS (2023) [2, 13, 14] [203] Yang, X., Li, Y., Lyu, S.: Exposing deep fakes using inconsistent head poses. In: ICASSP (2019) [5] [304] Yang, Y., Yang, Y., Guo, H., Xiong, R., Wang, Y., Liao, Y.: Urbangiraffe: Representing urban scenes as compositional generative neural feature fields. arXiv (2023) [5] [305] Yang, Z., Liang, J., Xu, Y., Zhang, X.Y., He, R.: Masked relation learning for deepfake detection. TIFS (2023) [2, 13, 14, 18] [306] Yao, X., Newson, A., Gousseau, Y., Hellier, P.: A latent transformer for disentangled face editing in images and videos. In: ICCV (2021) [203] Ye, Z., Zhong, T., Ren, Y., Yang, J., Li, W., Huang, J., Jiang, Z., He, J., Huang, R., Liu, J., et al.: Real3d-portrait: One-shot realistic 3d talking portrait synthesis. In: ICLR (2024) [307] Yildirim, A.B., Pohlivan, H., Bilecen, B.B., Dundar, A.: Diverse inpainting and editing with gan inversion. In: ICCV (2023) [6] Yin, Q., Lu, W., Li, B., Huang, J.: Dynamic difference learning with spatio-temporal correlation for deepfake video detection. TIFS (2023) [203] Yoo, S.M., Choi, T.M., Choi, J.W., Kim, J.H.: Fastswap: A lightweight one-stage framework for real-time face swapping. In: WACV (2023) [6] Yu, C., Lu, G., Zeng, Y., Sun, J., Liang, X., Li, H., Xu, Z., Xu, S., Zhang, W., Xu, H.: Towards high-fidelity text-guided 3d face generation and manipulation using only images. In: ICCV (2023) [12, 18] [70] Yu, H., Qu, Z., Yu, Q., Chen, J., Jiang, Z., Chen, Z., Zhang, S., Xu, J., Wu, F., Lv, C., et al.: Gaussianalker: Speaker-specific talking head synthesis via 3d gaussian splatting. arXiv (2024) [11] Yu, L., Xie, H., Zhang, Y.: Multimodal learning for temporally coherent talking face generation with articulator synergy. TMM (2021) [1* [313] Yu, N., Davis, L.S., Fritz, M.: Attributing fake images to gans: Learning and analyzing gan fingerprints. In: ICCV (2019) 15
* [314] Yu, W.Y., Po, L.M., Cheung, R.C., Zhao, Y., Xue, Y., Li, K.: Bidirectionally deformable motion modulation for video-based human pose transfer. In: ICCV (2023) 6
* [315] Yu, Z., Yin, Z., Zhou, D., Wang, D., Wong, F., Wang, B.: Talking head generation with probabilistic audio-to-visual diffusion priors. In: ICCV (2023) 11
* [316] Yuan, C., Liu, X., Zhang, Z.: The current status and progress of adversarial examples attacks. In: CISCE (2021) 6
* [317] Yuan, G., Li, M., Zhang, Y., Zheng, H.: Reliableswap: Boosting general face swapping via reliable supervision. arXiv (2023) 9
* [318] Zakharov, E., Ivakhnenko, A., Shysheya, A., Lempitsky, V.: Fast bi-layer neural synthesis of one-shot realistic head avatars. In: ECCV (2020) 9
* [319] Zakharov, E., Shysheya, A., Burkov, E., Lempitsky, V.: Few-shot adversarial learning of realistic neural talking head models. In: CVPR (2019) 9
* [320] Zeng, H., Zhang, W., Fan, C., Lv, T., Wang, S., Zhang, Z., Ma, B., Li, L., Ding, Y., Yu, X.: Flowface: Semantic flow-guided shape-aware face swapping. In: AAAI (2023) 7
* [321] Zeng, X., Pan, Y., Wang, M., Zhang, J., Liu, Y.: Realistic face reenactment via self-supervised disentangling of identity and pose. In: AAAI (2020) 10
* [322] Zhai, S., Liu, M., Li, Y., Gao, Z., Zhu, L., Nie, L.: Talking face generation with audio-deduced emotional landmarks. TNNLS (2023) 10, 17, 18
* [323] Zhai, Y., Luan, T., Doermann, D., Yuan, J.: Towards generic image manipulation detection with weakly-supervised self-consistency learning. In: ICCV (2023) 14
* [324] Zhang, B., Qi, C., Zhang, P., Zhang, B., Wu, H., Chen, D., Chen, Q., Wang, Y., Wen, F.: Metaportrait: Identity-preserving talking head generation with fast personalized adaptation. In: CVPR (2023) 9, 10
* [325] Zhang, B., Zhang, X., Cheng, N., Yu, J., Xiao, J., Wang, J.: Emotalker: Emotionally editable talking face generation via diffusion model. In: ICASSP (2024) 11
* [326] Zhang, C., Wang, C., Zhao, Y., Cheng, S., Luo, L., Guo, X.: Dr2: Disentangled recurrent representation learning for data-efficient speech video synthesis. In: WACV (2024) 1, 10, 11
* [327] Zhang, H., DAI, T., Xu, Y., Tai, Y.W., Tang, C.K.: Facender: Semantics-driven face reconstruction, prompt editing and relighting with diffusion models. In: NIPS (2024) 12
* [328] Zhang, H., Ren, Y., Chen, Y., Li, G., Li, T.H.: Exploiting multiple guidance from 3dmm for face reenactment. In: AAAIW (2023) 10
* [329] Zhang, J., Li, X., Li, J., Liu, L., Xue, Z., Zhang, B., Jiang, Z., Huang, T., Wang, Y., Wang, C.: Rethinking mobile block for efficient attention-based models. In: ICCV (2023) 4
* [330] Zhang, J., Li, X., Wan, Z., Wang, C., Liao, J.: Fdnerf: Few-shot dynamic neural radiance fields for face reconstruction and expression editing. In: SIGGRAPH (2022) 2, 12, 18
* [331] Zhang, J., Li, X., Wang, Y., Wang, C., Yang, Y., Liu, Y., Tao, D.: Eatformer: Improving vision transformer inspired by evolutionary algorithm. IJCV (2024) 4
* [332] Zhang, J., Liu, L., Xue, Z., Liu, Y.: App2face: Audio-guided face reenactment with auxiliary pose and blink signals. In: ICASSP (2020) 10
* [333] Zhang, J., Xu, C., Li, J., Han, Y., Wang, Y., Tai, Y., Liu, Y.: Scsenet: An efficient paradigm for learning simultaneously image colorization and super-resolution. In: AAAI (2022) 6
* [334] Zhang, J., Zeng, X., Wang, M., Pan, Y., Liu, L., Liu, Y., Ding, Y., Fan: Freenet: Multi-identity face reenactment. In: CVPR (2020) 9
* [335] Zhang, N., Paluri, M., Taigman, Y., Fergus, R., Bourdev, L.: Beyond frontal faces: Improving person recognition using multiple cues. In: CVPR (2015) 7
* [336] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: CVPR (2018) 5, 16
* [337] Zhang, W., Cun, X., Wang, X., Zhang, Y., Shen, X., Guo, Y., Shan, Y., Wang, F.: Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation. In: CVPR (2023) 10, 11
* [338] Zhang, W., Liu, Y., Dong, C., Qiao, Y.: Ranksrgan: Generative adversarial networks with ranker for image super-resolution. In: ICCV (2019) 15
* [339] Zhang, X., Sugano, Y., Fritz, M., Bulling, A.: Appearance-based gaze estimation in the wild. In: CVPR (2015) 9, 17
* [340] Zhang, X., Zhai, D., Li, T., Zhou, Y., Lin, Y.: Image inpainting based on deep learning: A review. Information Fusion (2023) 6
* [341] Zhang, Y., Zeng, H., Ma, B., Zhang, W., Zhang, Z., Ding, Y., Lv, T., Fan, C.: Flowface++: Explicit semantic flow-supervised end-to-end face swapping. arXiv (2023) 8, 17
* [342] Zhao, H., Zhou, W., Chen, D., Wei, T., Zhang, W., Yu, N.: Multi-attentional deepfake detection. In: CVPR (2021) 13, 14, 18
* [343] Zhao, J., Zhang, H.: Thin-plate spline motion model for image animation. In: CVPR (2022) 6, 15
* [344] Zhao, T., Xu, X., Xu, M., Ding, H., Xiong, Y., Xia, W.: Learning self-consistency for deepfake detection. In: ICCV (2021) 14, 15, 18
* [345] Zhao, W., Rao, Y., Shi, W., Liu, Z., Zhou, J., Lu, J.: Diffswap: High-fidelity and controllable face swapping via 3d-aware masked diffusion. In: CVPR (2023) 7, 8, 17
* [346] Zheng, G., Xu, Y.: Efficient face detection and tracking in video sequences based on deep learning. Information Sciences (2021) 4
* [347] Zheng, Y., Bao, J., Chen, D., Zeng, M., Wen, F.: Exploring temporal coherence for more general video face forgery detection. In: ICCV (2021) 13, 14, 18
* [348] Zhong, W., Fang, C., Cai, Y., Wei, P., Zhao, G., Lin, L., Li, G.: Identity-preserving talking face generation with landmark and appearance priors. In: CVPR (2023) 10, 11
* [349] Zhong, X., Huang, X., Wu, Z., Lin, G., Wu, Q.: Sara: Controllable makeup transfer with spatial alignment and region-adaptive normalization. arXiv (2023) 6, 16
* [350] Zhou, H., Liu, Y., Liu, Z., Luo, P., Wang, X.: Talking face generation by adversarially disentangled audio-visual representation. In: AAAI (2019) 11
* [351] Zhou, H., Sun, Y., Wu, W., Loy, C.C., Wang, X., Liu, Z.: Pose-controllable talking face generation by implicitly modularized audio-visual representation. In: CVPR (2021) 10, 11
* [352] Zhou, M., Liu, X., Yi, T., Bai, Z., Zhang, P.: A superior image inpainting scheme using transformer-based self-supervised attention gan model. Expert Systems with Applications (2023) 6* (353) Zhou, P., Han, X., Morariu, V.I., Davis, L.S.: Two-stream neural networks for tampered face detection. In: CVPRW (2017) 2
* (354) Zhou, P., Xie, L., Ni, B., Tian, Q.: Cips-3d++: End-to-end real-time high-resolution 3d-aware gans for gan inversion and stylization. TPAMI (2023) 12
* (355) Zhou, S., Xiao, T., Yang, Y., Feng, D., He, Q., He, W.: Genegan: Learning object transfiguration and attribute subspace from unpaired data. In: BMVC (2017) 12
* (356) Zhou, X., Yin, M., Chen, X., Sun, L., Gao, C., Li, Q.: Cross attention based style distribution for controllable person image synthesis. In: ECCV (2022) 6, 15
* (357) Zhou, Y., Han, X., Shechtman, E., Echevarria, J., Kalogerakis, E., Li, D.: Maklettalk: speaker-aware talking-head animation. ACM TOG (2020) 10, 11
* (358) Zhu, B., Fang, H., Sui, Y., Li, L.: Deepfakes for medical video de-identification: Privacy protection and diagnostic information preservation. In: AAAI (2020) 7, 8
* (359) Zhu, H., Wu, W., Zhu, W., Jiang, L., Tang, S., Zhang, L., Liu, Z., Loy, C.C.: Celebv-hq: A large-scale video facial attributes dataset. In: ECCV (2022) 5, 11
* (360) Zhu, J., Van Gool, L., Hoi, S.C.: Unsupervised face alignment by robust nonrigid mapping. In: ICCV (2009) 7
* (361) Zhu, Y., Li, Q., Wang, J., Xu, C.Z., Sun, Z.: One shot face swapping on megapixels. In: CVPR (2021) 7, 8
* (362) Zhu, Y., Zhao, W., Tang, Y., Rao, Y., Zhou, J., Lu, J.: Stableswap: Stable face swapping in a shared and controllable latent space. TMM (2024) 7
* (363) Zi, B., Chang, M., Chen, J., Ma, X., Jiang, Y.G.: Wide-deepfake: A challenging real-world dataset for deepfake detection. In: ACM MM (2020) 5, 14