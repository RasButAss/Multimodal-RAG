{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install langchain langchain-community langchain-openai torch torchvision torchaudio pillow transformers sentencepiece accelerate bitsandbytes ragas"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install git+https://github.com/VikParuchuri/marker faiss-cpu nougat-ocr git+https://github.com/facebookresearch/nougat"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install --no-deps unstructured chardet langdetect python-iso639 unstructured-client"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install chromadb langchain-chroma"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install ipywidgets jupyterlab-widgets==2.0.0b0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T01:12:17.272646Z","iopub.status.busy":"2024-07-12T01:12:17.272020Z","iopub.status.idle":"2024-07-12T01:12:27.555619Z","shell.execute_reply":"2024-07-12T01:12:27.554671Z","shell.execute_reply.started":"2024-07-12T01:12:17.272600Z"},"trusted":true},"outputs":[],"source":["import torch\n","from PIL import Image\n","from transformers import AutoModel, AutoTokenizer\n","model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5-int4', trust_remote_code=True)\n","tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5-int4', trust_remote_code=True)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_image_details(markdown_path, model, tokenizer):\n","    markdown_folder_path = os.path.dirname(markdown_path)\n","    markdown_file_name = os.path.basename(markdown_path).split('.')[0]\n","    markdown_with_image = f'{markdown_folder_path}/{markdown_file_name}_with_image.md'\n","\n","    with open(markdown_path, 'r') as read_file:\n","        with open(markdown_with_image, 'w') as write_file:\n","            chunk_size = 4000\n","            read_file_chunk = read_file.read(chunk_size)\n","            while len(read_file_chunk) > 0:\n","                image_pattern = r\"(?:[!]\\[(?P<caption>.*?)\\])\\((?P<image>.*?)\\)\"\n","                matches = re.finditer(image_pattern, read_file_chunk, re.MULTILINE)\n","                for match in matches:\n","                    print(match.group(1))\n","                    image = Image.open(f'{markdown_folder_path}/{match.group(1)}').convert('RGB')\n","                    question = read_file_chunk + (\"\\nBased from the above context explain in detail what the image is about? Please analyze the image thoroughly. The image may contain various forms of data representation such as charts, graphs, tables, etc.\"\n","                    \"For charts and graphs, identify and describe the type, axes, labels, data points, trends, and any significant peaks, troughs, or patterns. \"\n","                    \"Highlight any anomalies or outliers and discuss their possible implications. Extract and report all key numerical values. \"\n","                    \"For other types of images, describe all visible elements and their relationships in detail. Provide a clear and precise summary of the key features,\"\n","                    \"interpret the data where applicable, and make note of any notable observations or ambiguities.\" \n","                    \"It is crucial to extract all key numerical values if present in the image.\")\n","                    msgs = [{'role': 'user', 'content': question}]\n","                    response = model.chat(\n","                        image = image,\n","                        msgs = msgs,\n","                        tokenizer = tokenizer,\n","                        sampling = True,\n","                        temperature = 0.1,\n","                    )\n","                    print(response)\n","                    new_file_chunk = read_file_chunk[:match.end()] + f'\\n{response}\\n' + read_file_chunk[match.end():]\n","                    write_file.write(new_file_chunk)\n","                read_file_chunk = read_file.read(chunk_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import glob\n","pdfs = glob.glob('/kaggle/input/eval-files/*.pdf')\n","for file_path_pdf in pdfs:\n","    file_name = os.path.basename(file_path_pdf).split('.')[0]\n","    markdown_path = f'/kaggle/input/results/{file_name}/{file_name}.md'\n","    generate_image_details(markdown_path, model, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T03:12:08.211087Z","iopub.status.busy":"2024-07-12T03:12:08.210687Z","iopub.status.idle":"2024-07-12T03:12:08.252115Z","shell.execute_reply":"2024-07-12T03:12:08.250957Z","shell.execute_reply.started":"2024-07-12T03:12:08.211059Z"},"trusted":true},"outputs":[],"source":["import os\n","from typing import List\n","from langchain_community.document_loaders import UnstructuredMarkdownLoader, TextLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_chroma import Chroma\n","from langchain_core.documents import Document\n","from langchain_core.language_models import BaseLanguageModel\n","from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n","from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n","from langchain.schema.output_parser import StrOutputParser, BaseOutputParser\n","from langchain.retrievers import ParentDocumentRetriever, MultiQueryRetriever, ContextualCompressionRetriever, EnsembleRetriever\n","from langchain.storage import InMemoryStore\n","from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n","from langchain.document_transformers import EmbeddingsRedundantFilter\n","from langchain.retrievers.document_compressors import DocumentCompressorPipeline, EmbeddingsFilter\n","from marker.logger import configure_logging\n","from dotenv import load_dotenv\n","import re\n","\n","\n","load_dotenv()\n","configure_logging()\n","\n","class LineListOutputParser(BaseOutputParser[List[str]]):\n","    \"\"\"Output parser for a list of lines.\"\"\"\n","\n","    def parse(self, text: str) -> List[str]:\n","        lines = text.strip().split(\"\\n\")\n","        return lines\n","\n","class RAG:\n","    def __init__(self, visual_model, tokenizer):\n","        self.embeddings_model : BaseLanguageModel = AzureOpenAIEmbeddings(\n","            api_key=\"\",\n","            azure_endpoint=\"\",\n","            azure_deployment=\"\",\n","            openai_api_version=\"\"\n","        )\n","        self.chat_model : BaseLanguageModel = AzureChatOpenAI(\n","            api_key=\"\",\n","            azure_endpoint=\"\",\n","            azure_deployment=\"\",\n","            openai_api_version=\"\"\n","        )\n","        self.visual_model = visual_model\n","        self.tokenizer = tokenizer\n","\n","    def _load_chain(self):\n","        system_prompt = (\n","            \"You are an assistant for question-answering tasks. \"\n","            \"Use the following pieces of retrieved context to answer \"\n","            \"the question. If you don't know the answer, say that you \"\n","            \"don't know. Answer to the point\"\n","            \"\\n\\n\"\n","            \"{context}\\n\"\n","        )\n","\n","        prompt = ChatPromptTemplate.from_messages(\n","            [\n","                (\"system\", system_prompt),\n","                (\"human\", '{input}'),\n","            ]\n","        )\n","        \n","        def get_image_context(state):\n","            for doc in state['context']:\n","                text = doc.page_content\n","                image_pattern = r\"(?:[!]\\[(?P<caption>.*?)\\])\\((?P<image>.*?)\\)\"\n","                matches = re.finditer(image_pattern, text, re.MULTILINE)\n","                markdown_folder_path = os.path.dirname(doc.metadata['source'])\n","                question = state['input']\n","                for match in matches:\n","                    image = Image.open(f'{markdown_folder_path}/{match.group(1)}').convert('RGB')\n","                    question = text + (f'\\nBased from the above context explain in detail and answer the question.',\n","                    f'\\nQuestion : {question}\\nElaborate on all the details regarding the image structure,'\n","                    'colours and all the text in the image.'\n","                    \"Please analyze the image thoroughly. The image may contain various forms of data representation such as charts, graphs, tables, etc.\"\n","                    \"For charts and graphs, identify and describe the type, axes, labels, data points, trends, and any significant peaks, troughs, or patterns. \"\n","                    \"Highlight any anomalies or outliers and discuss their possible implications. Extract and report all key numerical values. \"\n","                    \"For other types of images, describe all visible elements and their relationships in detail. Provide a clear and precise summary of the key features,\"\n","                    \"interpret the data where applicable, and make note of any notable observations or ambiguities.\" \n","                    \"It is crucial to extract all key numerical values if present in the image.\")\n","                    msgs = [{'role': 'user', 'content': question}]\n","                    response = model.chat(\n","                        image = image,\n","                        msgs = msgs,\n","                        tokenizer = tokenizer,\n","                        sampling = True,\n","                        temperature = 0.1,\n","                    )\n","                    text = text + f'\\n{match.group(1)} : {response}\\n'\n","                doc.page_content = text\n","                print(doc.page_content)\n","            return state\n","\n","        def inspect(state):\n","            if len(state['context']) > 3:\n","                state['context'] = state['context'][:4]\n","            print([document.metadata['source'] for document in state['context']])\n","            return state\n","\n","        def convert_docs(state):\n","            state = [Document(page_content=doc.page_content, metadata=doc.metadata) for doc in state]\n","            return state\n","\n","        self.chain = ({\"input\" : RunnablePassthrough(), \n","                       \"context\" : (lambda x : x['input']) | self.retriever | RunnableLambda(convert_docs)}\n","        | RunnableLambda(inspect)\n","        | RunnableLambda(get_image_context)\n","        | prompt\n","        | self.chat_model\n","        | StrOutputParser())\n","   \n","\n","    def load_image_markdown(self, file_path_pdf):\n","        file_name = os.path.basename(file_path_pdf).split('.')[0]\n","        markdown_path = f'/kaggle/input/results/{file_name}/{file_name}.md'\n","        markdown_folder_path = os.path.dirname(markdown_path)\n","        markdown_file_name = os.path.basename(markdown_path).split('.')[0]\n","        markdown_with_image = f'{markdown_folder_path}/{markdown_file_name}_with_image.md'\n","        loader = UnstructuredMarkdownLoader(markdown_with_image)\n","        return loader\n","\n","    def load_marker_markdown(self, file_path_pdf):\n","        file_name = os.path.basename(file_path_pdf).split('.')[0]\n","        subfolder_path = f'/kaggle/input/eval-results/{file_name}'\n","        markdown_file_name = os.path.basename(subfolder_path)\n","        markdown_path = f'{subfolder_path}/{markdown_file_name}.md'\n","        loader = TextLoader(markdown_path)\n","        return loader\n","\n","    def load_nougat_markdown(self, file_path_pdf):\n","        file_name = os.path.basename(file_path_pdf).split('.')[0]\n","        output_folder_path = f'/kaggle/input/eval-results/{file_name}'\n","        markdown_path = f'{output_folder_path}/{file_name}.mmd'\n","        loader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\")\n","        return loader\n","    \n","    def create_vectorstore(self):\n","        self.vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=self.embeddings_model, persist_directory=\"/kaggle/working/chroma_chunks_1\")            \n","        \n","    def create_chroma_retriever(self):\n","        return self.vectorstore.as_retriever()\n","    \n","    def create_parent_document_retriever(self):\n","        child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n","        parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=400)\n","        return ParentDocumentRetriever(\n","            vectorstore=self.vectorstore,\n","            docstore=InMemoryStore(),\n","            child_splitter=child_splitter,\n","            parent_splitter=parent_splitter,\n","            search_kwags={'k' : 2}\n","        )\n","        \n","\n","    def load_file(self, parent_document_retriever, faiss_retriever):\n","        query_prompt = PromptTemplate(\n","            input_variables=[\"question\"],\n","            template=\"\"\"You are an AI language model assistant. Your task is to generate 3\n","                        different versions of the given user question to retrieve relevant documents from a vector\n","                        database. By generating multiple perspectives on the user question, your goal is to help\n","                        the user overcome some of the limitations of the distance-based similarity search.\n","                        Provide these alternative questions separated by newlines.\n","                        Original question: {question}\"\"\",\n","        )\n","        output_parser = LineListOutputParser()\n","        multiqueryretriever = MultiQueryRetriever(\n","            retriever = faiss_retriever,\n","            llm_chain = (query_prompt | self.chat_model | output_parser),\n","            parser_key = \"lines\"\n","        )\n","        ensemble_retriever = EnsembleRetriever(\n","            retrievers=[multiqueryretriever,parent_document_retriever], weights=[0.4, 0.6]\n","        )\n","        redundant_filter = EmbeddingsRedundantFilter(embeddings=self.embeddings_model, similarity_threshold=0.85)\n","        relevant_filter = EmbeddingsFilter(embeddings=self.embeddings_model, similarity_threshold=0.53)\n","        pipeline_compressor = DocumentCompressorPipeline(\n","            transformers=[redundant_filter,relevant_filter]\n","        )\n","        self.retriever = ContextualCompressionRetriever(\n","            base_compressor=pipeline_compressor, base_retriever=ensemble_retriever\n","        )\n","        self._load_chain()\n","\n","\n","    def get_response(self, input):\n","\n","        response = self.chain.invoke({\"input\" : input})\n","        context = [doc.page_content for doc in self.retriever.invoke(input)]\n","        return response, context"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T03:12:11.104397Z","iopub.status.busy":"2024-07-12T03:12:11.103709Z","iopub.status.idle":"2024-07-12T03:12:11.162612Z","shell.execute_reply":"2024-07-12T03:12:11.161723Z","shell.execute_reply.started":"2024-07-12T03:12:11.104339Z"},"trusted":true},"outputs":[],"source":["import glob\n","import torch \n","from langchain_community.vectorstores import utils as chromautils\n","rag_instance = RAG(model, tokenizer)\n","rag_instance.create_vectorstore()\n","parent_document_retriever = rag_instance.create_parent_document_retriever()\n","chroma_retriever = rag_instance.create_chroma_retriever()\n","pdfs = glob.glob('/kaggle/input/eval-files/*.pdf')\n","print(pdfs)\n","for file_path_pdf in pdfs:\n","    loader_marker = rag_instance.load_marker_markdown(file_path_pdf)\n","    docs = chromautils.filter_complex_metadata(loader_marker.load())\n","    parent_document_retriever.add_documents(docs)\n","    \n","for file_path_pdf in pdfs:\n","    loader_nougat = rag_instance.load_nougat_markdown(file_path_pdf)\n","    docs = chromautils.filter_complex_metadata(loader_nougat.load())\n","    parent_document_retriever.add_documents(docs)\n","\n","for file_path_pdf in pdfs:\n","    loader_image = rag_instance.load_image_markdown(file_path_pdf)\n","    docs = chromautils.filter_complex_metadata(loader_image.load())\n","    parent_document_retriever.add_documents(docs)\n","    \n","rag_instance.load_file(parent_document_retriever, chroma_retriever)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","questions = pd.read_excel('/kaggle/input/eval-questions/Test_questions_IIT4th.xlsx')\n","\n","answer = {'question' : [], 'answer' : [], 'context' : []}\n","for question in questions['Questions']:\n","    response, context = rag_instance.get_response(question)\n","    print(question)\n","    print(response)\n","    print(context)\n","    answer['question'].append(question)\n","    answer['answer'].append(response)\n","    answer['context'].append(context)\n","\n","answer_dataframe = pd.DataFrame(answer)\n","answer_dataframe.to_csv('/kaggle/working/samarth-final.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# SOME SAMPLE REPONSES FOR THE GIVEN QUESTIONS"]},{"cell_type":"markdown","metadata":{},"source":["In the Scaled Dot-Product Attention mechanism, when the queries (Q) and keys (K) are input, the flow involves the following steps:\n","\n","1. **Input**: The input consists of queries and keys, which are vectors of dimension dk.\n","2. **Dot Products**: The attention mechanism computes the dot products of the query with all keys. This is a multiplication operation between the query and key vectors.\n","3. **Scaling**: Each dot product is then divided by the square root of dk. This scaling step is crucial as it helps in preventing the dot products from growing too large.\n","4. **Softmax Function**: After scaling, a softmax function is applied to obtain the weights on the values. The softmax function is used to normalize the weights so that they form a probability distribution over the possible outputs.\n","\n","After these steps, the final output is computed as a weighted sum of the values, where the weight assigned to each value is determined by the compatibility function of the query with the corresponding key."]},{"cell_type":"markdown","metadata":{},"source":["In the Scaled Dot-Product Attention mechanism, when the queries (Q) and keys (K) are input, the flow involves the following steps:\n","\n","1. **Computing Q, K, V**: The input queries (Q) and keys (K) are used to compute the query and key matrices. These matrices are typically derived from the input data through some form of transformation or encoding.\n","\n","2. **Computing Attention Weights**: The dot products of the query with all keys are computed, each is divided by âˆšdk, and then a softmax function is applied to obtain the weights on the values. This step involves taking the dot product of the query and key matrices, dividing by the square root of the key dimension (dk), and applying a softmax function to ensure that the attention weights are non-negative and sum up to one.\n","\n","3. **Computing Attention Output**: After computing the attention weights, the attention output is computed by multiplying the weight matrix with the value matrix, producing the final output of the attention mechanism.\n","\n","This process results in the computation of the attention weights based on the input queries (Q) and keys (K), which are then used to derive the attention output."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T01:14:02.577284Z","iopub.status.busy":"2024-07-12T01:14:02.575879Z","iopub.status.idle":"2024-07-12T01:14:03.749722Z","shell.execute_reply":"2024-07-12T01:14:03.748775Z","shell.execute_reply.started":"2024-07-12T01:14:02.577247Z"},"trusted":true},"outputs":[],"source":["pdf_names = [os.path.basename(file_path_pdf).split(sep='.')[0] for file_path_pdf in pdfs]\n","documents = []\n","for dir_name in pdf_names:\n","    markdowns = glob.glob(f'/kaggle/input/eval-results/{dir_name}/{dir_name}.md')\n","    math_markdowns = glob.glob(f'/kaggle/input/eval-results/{dir_name}/{dir_name}.mmd')\n","    markdowns.extend(math_markdowns)\n","    for markdown in markdowns:\n","        markdown_documents = UnstructuredMarkdownLoader(markdown, mode='elements')\n","        documents.extend(markdown_documents.load())\n","len(documents)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T01:14:09.313006Z","iopub.status.busy":"2024-07-12T01:14:09.312151Z","iopub.status.idle":"2024-07-12T01:20:37.171459Z","shell.execute_reply":"2024-07-12T01:20:37.170593Z","shell.execute_reply.started":"2024-07-12T01:14:09.312971Z"},"trusted":true},"outputs":[],"source":["from ragas.testset.generator import TestsetGenerator\n","from ragas.testset.evolutions import simple, reasoning, multi_context\n","import nest_asyncio\n","\n","nest_asyncio.apply()\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=400)\n","splits = text_splitter.split_documents(documents)\n","# generator with openai models\n","generator_llm = rag_instance.chat_model\n","embeddings = rag_instance.embeddings_model\n","\n","generator = TestsetGenerator.from_langchain(\n","    generator_llm=generator_llm,\n","    critic_llm=generator_llm,\n","    embeddings=embeddings,\n",")\n","\n","testset = generator.generate_with_langchain_docs(splits, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T01:22:30.317667Z","iopub.status.busy":"2024-07-12T01:22:30.317297Z","iopub.status.idle":"2024-07-12T01:22:30.361300Z","shell.execute_reply":"2024-07-12T01:22:30.360331Z","shell.execute_reply.started":"2024-07-12T01:22:30.317641Z"},"trusted":true},"outputs":[],"source":["testset.to_pandas()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T01:28:55.577951Z","iopub.status.busy":"2024-07-12T01:28:55.577160Z","iopub.status.idle":"2024-07-12T01:30:19.160280Z","shell.execute_reply":"2024-07-12T01:30:19.159422Z","shell.execute_reply.started":"2024-07-12T01:28:55.577916Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset\n","\n","questions = testset.to_pandas()[\"question\"].to_list()\n","ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()\n","\n","data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n","\n","for query in questions:\n","    data[\"question\"].append(query)\n","    data[\"answer\"].append(rag_instance.get_response(query))\n","    data[\"contexts\"].append([doc.page_content for doc in rag_instance.retriever.invoke(query)])\n","\n","dataset = Dataset.from_dict(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T01:34:33.772277Z","iopub.status.busy":"2024-07-12T01:34:33.771893Z","iopub.status.idle":"2024-07-12T01:35:54.515946Z","shell.execute_reply":"2024-07-12T01:35:54.514489Z","shell.execute_reply.started":"2024-07-12T01:34:33.772245Z"},"trusted":true},"outputs":[],"source":["from ragas import evaluate\n","from ragas.metrics import (\n","    faithfulness,\n","    answer_relevancy,\n","    context_relevancy,\n","    context_recall,\n","    context_precision,\n","    answer_similarity,\n",")\n","\n","nest_asyncio.apply()\n","\n","result = evaluate(\n","    dataset = dataset,\n","    llm=generator_llm,\n","    embeddings=embeddings,\n","    metrics=[\n","        context_relevancy,\n","        context_precision,\n","        context_recall,\n","        faithfulness,\n","        answer_relevancy,\n","        answer_similarity\n","    ],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T01:25:40.267216Z","iopub.status.busy":"2024-07-12T01:25:40.266868Z","iopub.status.idle":"2024-07-12T01:25:40.305029Z","shell.execute_reply":"2024-07-12T01:25:40.304074Z","shell.execute_reply.started":"2024-07-12T01:25:40.267190Z"},"trusted":true},"outputs":[],"source":["result.to_pandas()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5370922,"sourceId":8928645,"sourceType":"datasetVersion"},{"datasetId":5373421,"sourceId":8932067,"sourceType":"datasetVersion"},{"datasetId":5375014,"sourceId":8934341,"sourceType":"datasetVersion"},{"datasetId":5375210,"sourceId":8934636,"sourceType":"datasetVersion"},{"datasetId":5375267,"sourceId":8934704,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
